# 기계학습 5주차

# **05-1 결정 트리(Decision Tree)**

결정 트리는 **데이터를 학습하여 분류 또는 회귀 문제를 해결하는 모델**이다.

데이터의 특징을 이용하여 **트리 형태로 분할하면서 학습하는 알고리즘**이다.

설명이 쉬우며 시각적으로도 이해하기 좋은 모델이지만, **과적합(overfitting)** 에 주의해야 한다.

---

## **1. 결정 트리 개념**

결정 트리는 **트리 구조(Tree Structure)** 를 이용하여 데이터를 분류하는 지도 학습(Supervised Learning) 알고리즘이다.

- **노드(Node)** : 데이터의 특정 특징(feature)을 기준으로 분할
- **가지(Branch)** : 특정 조건에 따라 데이터가 나뉘는 경로
- **리프 노드(Leaf Node)** : 최종적으로 분류된 결과

**결정 트리의 특징**

1. **설명이 쉬움** → 트리 구조로 데이터를 나누는 방식이 직관적
2. **비선형 관계 학습 가능** → 데이터의 복잡한 패턴을 학습 가능
3. **다양한 문제에 적용 가능** → 분류(Classification)와 회귀(Regression) 모두 사용
4. **과적합(Overfitting) 가능성 있음** → 깊이가 깊어질수록 훈련 데이터에 과적합될 가능성이 높아짐

**결정 트리의 사용 사례**

- 스팸 메일 분류
- 질병 진단
- 고객 이탈 예측
- 신용카드 사기 탐지

---

## **2. 결정 트리를 사용한 와인 분류**

와인의 알코올 도수, 당도, pH 값을 이용해 **레드 와인과 화이트 와인을 분류하는 문제**를 해결해본다.

### **1. 데이터 로드 및 탐색**

```python
import pandas as pd

# 와인 데이터 로드
wine = pd.read_csv('https://bit.ly/wine_csv_data')

# 데이터 확인
wine.head()
```

출력 결과 :

![스크린샷 2025-02-02 15.22.04.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%205%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%2018e77f5ae6fa80219083ce305fabbb16/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2025-02-02_15.22.04.png)

**데이터 설명**

- **alcohol** : 알코올 도수
- **sugar** : 당도
- **pH** : 산도
- **class** : 0(레드 와인), 1(화이트 와인)

---

### **2. 데이터 전처리 (결측값 확인 및 통계 요약)**

```python

# 데이터 정보 확인
wine.info()

```

출력 결과 :

![스크린샷 2025-02-02 15.23.03.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%205%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%2018e77f5ae6fa80219083ce305fabbb16/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2025-02-02_15.23.03.png)

**결측값(누락된 값) 확인**

- 모든 열이 6497개로 **누락된 데이터 없음**

```python

# 데이터 요약 통계
wine.describe()

```

출력 결과 :

![스크린샷 2025-02-02 15.40.07.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%205%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%2018e77f5ae6fa80219083ce305fabbb16/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2025-02-02_15.40.07.png)

**통계 요약 확인**

- 평균(mean), 표준편차(std), 최소(min), 최대(max), 사분위수(25%, 50%, 75%) 확인 가능

---

### **3.  데이터 준비 (입력값 & 타겟값 분리)**

```python
# 입력 데이터(X)와 타겟 데이터(y) 분리
data = wine[['alcohol', 'sugar', 'pH']].to_numpy()
target = wine['class'].to_numpy()
```

- **입력 데이터 (X)** : 알코올 도수, 당도, pH 값
- **타겟 데이터 (y)** : 와인의 종류 (0: 레드, 1: 화이트)

---

### **4.  훈련 세트 & 테스트 세트 분할**

```python
from sklearn.model_selection import train_test_split

# 데이터 나누기 (80% 훈련, 20% 테스트)
train_input, test_input, train_target, test_target = train_test_split(
    data, target, test_size=0.2, random_state=42
)

print(train_input.shape, test_input.shape)
# 결과 : (5197, 3), (1300, 3)
```

- **훈련 데이터(train_input, train_target)** : 모델 학습용
- **테스트 데이터(test_input, test_target)** : 모델 평가용

---

### **5. 데이터 정규화 (표준화)**

```python
from sklearn.preprocessing import StandardScaler

# 표준화 적용
ss = StandardScaler()
ss.fit(train_input)
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)
```

**표준화(Standardization)** :

- 입력값(알코올 도수, 당도, pH)의 단위를 맞추기 위해 **평균 0, 분산 1로 변환**
- 머신러닝 모델의 성능을 향상시키는 역할

---

### **6. 로지스틱 회귀 모델 학습**

```python
from sklearn.linear_model import LogisticRegression

# 로지스틱 회귀 모델 생성 및 학습
lr = LogisticRegression()
lr.fit(train_scaled, train_target)

# 훈련 및 테스트 정확도 출력
print(lr.score(train_scaled, train_target))  
print(lr.score(test_scaled, test_target))  

# 결과 : 0.7808350971714451
#       0.7776923076923077
```

**로지스틱 회귀 단점**

- 선형 모델이므로 데이터가 복잡할 경우 성능이 떨어질 수 있음
- 설명이 어렵고 직관적으로 이해하기 어려움

---

## **3. 결정 트리 모델 학습**

로지스틱 회귀의 단점을 극복하기 위해 **결정 트리 모델**을 적용.

### **1. 결정 트리 모델 훈련**

```python
from sklearn.tree import DecisionTreeClassifier

# 결정 트리 모델 생성 및 학습
dt = DecisionTreeClassifier(random_state=42)
dt.fit(train_scaled, train_target)

# 훈련 및 테스트 정확도 출력
print(dt.score(train_scaled, train_target))
print(dt.score(test_scaled, test_target))

# 결과 : 0.996921300750433
#       0.8592307692307692
```

**결정 트리의 문제점**

- 훈련 데이터에 **과적합(Overfitting)** 되어 테스트 데이터 성능이 낮아질 수 있음

---

### **2. 결정 트리 시각화**

```python
import matplotlib.pyplot as plt
from sklearn.tree import plot_tree

# 결정 트리 시각화
plt.figure(figsize=(10, 7))
plot_tree(dt)
plt.show()
```

출력 결과 : 

![스크린샷 2025-02-02 15.47.10.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%205%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%2018e77f5ae6fa80219083ce305fabbb16/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2025-02-02_15.47.10.png)

**트리 구조를 시각적으로 확인 가능**

**트리의 깊이를 제한**

```python
plt.figure(figsize=(10,7))
plot_tree(dt, max_depth=1, filled=True, feature_names=[' alcohol',
'sugar', 'ph'])
plt.show()
```

출력 결과 : 

![스크린샷 2025-02-02 15.49.22.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%205%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%2018e77f5ae6fa80219083ce305fabbb16/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2025-02-02_15.49.22.png)

불순도(gini, 지니 불순도)

- decision TreeClassifier 클래스의criterion 매개변수의 기본값이 gini
- criterion 매개변수의 용도는 노드에서 데이터를 분할할 기준을 정함
- 지니 불순도 = 1 - ( 음성 클래스 비율² + 양성 클래스 비율² )
    - 1 - ((1258/ 5197)² + (3939/ 5197)**²**) = 0.367
    - 1  - ((50/ 100)² + (50/ 100)²) = 0.5
    - 1 - ((0/ 100)² + (100/ 100)²) = 0
- 불순도의 차이 = 부모의 불순도 - ( 왼쪽 노드 샘플 수 / 부모의 샘플 수 ) x 왼쪽 노드 불순도
( 오른쪽 노드 샘플 수 / 부모의 샘플 수 ) x 오른쪽 노드 불순도
    - 0.367- (2922/ 5197) × 0.481 - (2275/ 5197) x 0.069 = 0.066
- 루트 노드의 엔트로피 불순도 =  - 음성 클래스 비율 x log₂ ( 음성 클래스 비율 ) - 양성 클래스 비율 x log₂ ( 양성 클래스 비율 )
    - -(1258/ 5197) x log₂(1258 / 5197) - (3939 / 5197) x log₂ (3939 / 5197) = 0. 798

---

### **3. 가지치기 (과적합 방지)**

```python
# 최대 깊이를 3으로 제한하여 과적합 방지
dt = DecisionTreeClassifier(max_depth=3, random_state=42)
dt.fit(train_scaled, train_target)

# 훈련 및 테스트 정확도 출력
print(dt.score(train_scaled, train_target))
print(dt.score(test_scaled, test_target))

# 결과 : 0.8454877814123533
#       0.8415384615384616
```

**가지치기(Pruning) 효과**

- 과적합 방지
- 트리의 복잡도를 줄여 일반화 성능 향상

---

### **4. 결정 트리 중요도 평가**

```python
plt. figure(figsize=(20,15))
plot_tree(dt, filled=True, feature_names=['alcohol', 'sugar', 'pH'])
plt.show()
```

출력 결과 : 

![스크린샷 2025-02-02 16.03.43.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%205%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%2018e77f5ae6fa80219083ce305fabbb16/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2025-02-02_16.03.43.png)

```python
# 각 특징의 중요도 출력
print(dt.feature_importances_)
```

**출력 결과 예시**

```python
[0.12345626, 0.86862934, 0.0079144]
```

**의미**

- **당도(sugar)** 가 가장 중요한 특징으로 작용
- **알코올 도수(alcohol)** 는 상대적으로 덜 중요
- **pH** 값은 거의 영향 없음

---

## **4. 결정 트리 vs 로지스틱 회귀 비교**

| 모델 | 장점 | 단점 |
| --- | --- | --- |
| **로지스틱 회귀** | 계산이 빠르고 해석이 쉬움 | 데이터가 복잡할 경우 성능 저하 |
| **결정 트리** | 직관적이고 설명이 쉬움 | 과적합 위험 |
- **결정 트리는 데이터의 특성을 잘 반영하지만, 과적합을 방지하기 위한 조치가 필요하다.**
- **가지치기(pruning), 트리 깊이 제한 등의 기법을 적용하면 일반화 성능을 향상시킬 수 있다.**

---

# **05-2 교차 검증과 그리드 서치**

모델의 성능을 올바르게 평가하고, 최적의 하이퍼파라미터를 찾기 위해 **교차 검증(Cross Validation)과 그리드 서치(Grid Search)** 를 활용한다.

---

## **1. 검증 세트(Validation Set) 개념**

모델을 평가할 때 **훈련 데이터만 사용하면 과적합(Overfitting)** 이 발생할 수 있다. 이를 방지하기 위해 **훈련 세트를 나누어 검증 세트를 만드는 방법**이 있다.

- **훈련 세트(Training Set)** → 모델을 학습시키는 데이터
- **검증 세트(Validation Set)** → 모델의 성능을 평가하는 데이터
- **테스트 세트(Test Set)** → 최종 모델의 성능을 평가하는 데이터

**데이터 분할 비율**

- **훈련 80% + 테스트 20%** (기본적인 방식)
- **훈련 60% + 검증 20% + 테스트 20%** (검증 세트 활용)

---

## **2. 검증 세트 만들기**

```python
import pandas as pd

# 와인 데이터 로드
wine = pd.read_csv('https://bit.ly/wine_csv_data')

# 특성(입력 데이터)과 타겟(정답 데이터) 분리
data = wine[['alcohol', 'sugar', 'pH']].to_numpy()
target = wine['class'].to_numpy()
```

- **입력 데이터 (X)** : 알코올 도수, 당도, pH
- **타겟 데이터 (y)** : 와인의 종류 (0: 레드, 1: 화이트)

```python
from sklearn.model_selection import train_test_split

# 훈련 세트와 테스트 세트 분할
train_input, test_input, train_target, test_target = train_test_split(
    data, target, test_size=0.2, random_state=42
)

# 훈련 세트에서 다시 검증 세트를 분리 (훈련 80%, 검증 20%)
sub_input, val_input, sub_target, val_target = train_test_split(
    train_input, train_target, test_size=0.2, random_state=42)
```

- **훈련 데이터**: `sub_input, sub_target`
- **검증 데이터**: `val_input, val_target`
- **테스트 데이터**: `test_input, test_target`

```python
# 데이터 크기 확인
print(sub_input.shape, val_input.shape)

# 결과 : (4157, 3), (1040, 3)
```

→ 훈련 데이터 4,157개 / 검증 데이터 1,040개

**모델을 만들어 평가** (sub input, sub arget 과 val_input, val arget 사용)

```python
from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier(random_state=42)
dt. fit(sub_input, sub_target)
print(dt.score(sub_input, sub_target))
print(dt.score(val_input, val_target))

# 출력 결과 : 0.9971133028626413
#           0.864423076923077
```

---

## **3. 교차 검증 (Cross Validation)**

검증 세트를 만들면 모델의 성능을 평가할 수 있지만, **검증 세트가 특정 데이터에 의존할 위험**이 있다. 이를 해결하기 위해 **교차 검증(Cross Validation)** 을 사용한다.

### **교차 검증 개념**

교차 검증은 훈련 세트를 여러 개로 나누어 번갈아 가며 검증하는 방식이다.

대표적인 방법: **k-폴드 교차 검증(k-fold Cross Validation)**

- 데이터를 **k개로 분할한 후 k번 반복하여 모델을 평가**한다.
- 보통 **5-폴드(k=5) 또는 10-폴드(k=10) 교차 검증**을 사용한다.

### **1. 3-폴드 교차 검증 적용**

```python
from sklearn.model_selection import cross_validate
from sklearn.tree import DecisionTreeClassifier

# 결정 트리 모델 생성
dt = DecisionTreeClassifier(random_state=42)

# 교차 검증 수행
scores = cross_validate(dt, train_input, train_target)
print(scores)

# 결과 : {'fit_time': array([0.01334453, 0.01186419, 0.00783849, 0. 0077858, 0.00726461]), 'score_time': array([0.00085783, 0.00062561, 0.00061512, 0. 00063181, 0.00067616]), 'test_score': array([0.86923077, 0.84615385, 0.87680462, 0.84889317, 0.83541867])}
```

**test_score** : 세 번의 교차 검증 결과

```python
import numpy as np

# 교차 검증 평균 성능 확인
print(np.mean(scores['test_score']))

# 결과 :  0.855300214703487
```

→ 3-폴드 교차 검증의 평균 정확도: **85.5%**

---

### **2. 10-폴드 교차 검증 (Stratified K-Fold 적용)**

```python
from sklearn.model_selection import StratifiedKFold

from sklearn model_selection import StratifiedKFold
scores = cross_validate(dt, train_input, train_target, cv=StratifiedKFold())
print(np. mean(scores['test_score']))

# 결과 : 0.855300214703487

splitter = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
scores = cross_validate(dt, train_input, train_target, cv=splitter)
print (np.mean(scores[' test_score']))

# 결과 : 0.8574181117533719
```

→ **10-폴드 교차 검증의 평균 정확도: 85.7%**

---

## **4. 하이퍼파라미터 튜닝 (Grid Search & Random Search)**

**하이퍼파라미터(Hyperparameter)** 는 모델이 학습할 때 사람이 직접 설정해야 하는 값이다.

예: `max_depth`, `min_samples_split`, `min_impurity_decrease` 등

### **1. 그리드 서치 (Grid Search)**

그리드 서치는 **미리 설정한 하이퍼파라미터 후보들에 대해 모든 조합을 평가**하는 방식이다.

```python
from sklearn.model_selection import GridSearchCV

# 하이퍼파라미터 후보 설정
params = {'min_impurity_decrease': [0.0001, 0.0002, 0.0003, 0.0004, 0.0005]}

# 그리드 서치 객체 생성
gs = GridSearchCV(DecisionTreeClassifier(random_state=42), params, n_jobs=-1)

# 훈련 수행
gs.fit(train_input, train_target)

dt = gs.best_estimator_
print(dt. score(train_input, train_target))

# 결과 : 0.9615162593804117

```

**최적의 하이퍼파라미터 찾기**

```python
print(gs.best_params_)

# 결과 : {'min_impurity_decrease': 0.0001}

print(gs.cv_results_['mean_test_score'])

# 결과 : [0.86819297, 0.86453617, 0.86492226, 0.86780891, 0.86761605]

best_index = np.argmax(gs.cv_results_['mean_test_score'])
print(gs.cv_results_['params'][best_index])

# 결과 : {'min_impurity_decrease': 0.0001}

params = {'min_impurity_decrease : np.arange(0.0001, 0.001, 0.0001),
'max_depth' : range(5, 20, 1),
'min_samples_split': range(2, 100, 10) }

gs = GridSearchCV(DecisionTreeClassifier(random_state=42), params, n_jobs=-1)
gs. fit(train_input, train_target)

print(gs.best_params_)
# 결과 : {'max_depth': 14, 'min_impurity _decrease': 0.0004, 'min_samples_split': 12}
```

**최고 정확도 확인**

```python
print(np.max(gs.cv_results_['mean_test_score']))

# 결과 : 0.868
```

→ **최적의 파라미터로 86.8% 성능 달성**

---

### **2. 랜덤 서치 (Randomized Search)**

랜덤 서치는 **일정 범위에서 랜덤하게 하이퍼파라미터를 샘플링**하여 탐색하는 방법.

```python
from scipy.stats import uniform, randint

rgen = randint(0, 10)
rgen. rvs (10)
# 결과 : array ([6, 4, 2, 2, 7, 7, 0, 0, 5, 4])

np. unique(rgen.rvs(1000), return_counts=True)
# 결과 : (array ([0, 1, 2, 3, 4, 5, 6, 7, 8, 91), array([ 98, 94, 99, 93, 93, 92, 111, 118, 105, 97]))

ugen = uniform(0, 1)
ugen. rvs (10)
# 결과 : array ([0.12982148, 0.32130647, 0.22468098, 0.09345374, 0.43188927, 0.69791727, 0.81250121, 0.54913255, 0.00552007, 0.52386115])

# 랜덤한 하이퍼파라미터 분포 설정
params = {
    'min_impurity_decrease': uniform(0.0001, 0.001),
    'max_depth': randint(20, 50),
    'min_samples_split': randint(2, 25),
    'min_samples_leaf': randint(1, 25)
}

from sklearn.model_selection import RandomizedSearchCV

# 랜덤 서치 객체 생성
gs = RandomizedSearchCV(
    DecisionTreeClassifier(random_state=42), params,
    n_iter=100, n_jobs=-1, random_state=42
)

# 훈련 수행
gs.fit(train_input, train_target)
```

**최적의 하이퍼파라미터 출력**

```python
print(gs.best_params_)

# 결과 : {'max_depth': 39, 'min_impurity_decrease': 0.00034, 'min_samples_split': 13, 'min_samples_leaf': 2}
```

**최고 정확도 확인**

```python
print(np.max(gs.cv_results_['mean_test_score']))

# 결과 : 0.869

dt = gs. best_estimator_
print(dt. score(test_input, test_target))
# 결과 : 0.86
```

→ **랜덤 서치로 86.9% 성능 달성**

---

# **05-3 트리의 앙상블**

결정 트리(Decision Tree)를 여러 개 결합하여 성능을 향상시키는 **앙상블 학습(Ensemble Learning)** 에 대해 배운다.

---

## **1. 앙상블 학습(Ensemble Learning)이란?**

단일 모델보다 여러 개의 모델을 조합하여 성능을 높이는 방법

- 과적합을 줄이고 일반화 성능을 향상
- 대표적인 방법: **랜덤 포레스트, 엑스트라 트리, 그래디언트 부스팅**

---

## **2. 정형 데이터와 비정형 데이터**

- **정형 데이터 (Structured Data)**→ 테이블 형태로 정리된 데이터 (예: CSV 파일, 데이터베이스)
- **비정형 데이터 (Unstructured Data)**→ 구조화되지 않은 데이터 (예: 이미지, 텍스트, 오디오)

**랜덤 포레스트(Random Forest)는 주로 정형 데이터에서 뛰어난 성능**을 보임

---

## **3. 랜덤 포레스트(Random Forest)**

랜덤 포레스트는 **여러 개의 결정 트리를 앙상블하여 학습하는 방법**

- 각각의 트리가 독립적으로 학습하여 예측을 수행
- 훈련 데이터를 랜덤하게 샘플링하여 트리마다 다른 데이터셋 사용 (**부트스트랩 샘플링**)
- 과적합을 방지하고 안정적인 성능을 보장

### **랜덤 포레스트 구현 (교차 검증 포함)**

```python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, cross_validate
from sklearn.ensemble import RandomForestClassifier

# 데이터 로드
wine = pd.read_csv('https://bit.ly/wine_csv_data')
data = wine[['alcohol', 'sugar', 'pH']].to_numpy()
target = wine['class'].to_numpy()

# 데이터 분할
train_input, test_input, train_target, test_target = train_test_split(
    data, target, test_size=0.2, random_state=42
)

# 랜덤 포레스트 모델 생성 및 교차 검증
rf = RandomForestClassifier(n_jobs=-1, random_state=42)
scores = cross_validate(rf, train_input, train_target, return_train_score=True, n_jobs=-1)

# 훈련 세트 & 테스트 세트 점수 출력
print(np.mean(scores['train_score']), np.mean(scores['test_score']))

# 결과 : 0. 9973541965122431, 0.8905151032797809
```

→ 훈련 세트에서는 거의 완벽한 성능을 보이고, 테스트 세트에서도 높은 성능 유지

**특성 중요도(Feature Importance) 확인**

```python
rf.fit(train_input, train_target)
print(rf.feature_importances_)

# 출력 : [0.23167441, 0.50039841, 0.26792718]
```

→ **당도(sugar)가 가장 중요한 특성**임을 알 수 있음

**OOB 점수(Out-of-Bag Score) 활용**

```python
rf = RandomForestClassifier(oob_score=True, n_jobs=-1, random_state=42)
rf.fit(train_input, train_target)
print(rf.oob_score_)

# 출력 : 0.8934000384837406
```

→ **OOB 점수**를 활용하면 별도의 검증 세트 없이 모델 성능을 평가 가능

---

## **4. 엑스트라 트리(Extra Trees)**

랜덤 포레스트와 비슷하지만 **부트스트랩 샘플링을 사용하지 않음**

- 모든 훈련 데이터를 사용하여 트리 학습
- 분할 기준을 무작위로 선택하여 과적합 방지

### **엑스트라 트리 구현 (교차 검증 포함)**

```python
from sklearn.ensemble import ExtraTreesClassifier

# 엑스트라 트리 모델 생성
et = ExtraTreesClassifier(n_jobs=-1, random_state=42)

# 교차 검증 수행
scores = cross_validate(et, train_input, train_target, return_train_score=True, n_jobs=-1)

# 훈련 세트 & 테스트 세트 점수 출력
print(np.mean(scores['train_score']), np.mean(scores['test_score']))

# 출력 : 0.9974503966084433, 0.8887848893166506
```

→ 랜덤 포레스트와 비슷한 성능이지만, 과적합 방지 효과가 더 큼

**특성 중요도 확인**

```python
et.fit(train_input, train_target)
print(et.feature_importances_)

# 출력 : [0.20183568, 0.52242907, 0.27573525]
```

→ **당도(sugar)의 중요도가 가장 큼**

---

## **5. 그래디언트 부스팅(Gradient Boosting)**

**깊이가 얕은 결정 트리를 여러 개 쌓아서 학습하는 방식**

- 이전 트리의 오류를 보완하는 방식으로 학습
- 학습률(Learning Rate)과 트리 개수를 조절하여 최적화

### **그래디언트 부스팅 구현**

```python
from sklearn.ensemble import GradientBoostingClassifier

# 그래디언트 부스팅 모델 생성
gb = GradientBoostingClassifier(random_state=42)

# 교차 검증 수행
scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)

# 훈련 세트 & 테스트 세트 점수 출력
print(np.mean(scores['train_score']), np.mean(scores['test_score']))

# 결과 : 0.8881086892152563, 0.8720430147331015
```

→ **과적합이 적고 안정적인 성능**을 보임

**트리 개수 & 학습률 조정**

```python
gb = GradientBoostingClassifier(n_estimators=500, learning_rate=0.2, random_state=42)
scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))

# 결과 : 0.9464595437171814, 0.8780082549788999
```

→ 트리 개수를 늘리면 성능 향상 가능, 하지만 과적합 위험 있음

**특성 중요도 확인**

```python
gb. fit(train_input, train_target)
print(gb. feature_importances_)

# 출력 : [0. 15872278, 0.68010884, 0.16116839]
```

→ **당도(sugar)의 중요도가 큼**

---

## **6. 히스토그램 기반 그래디언트 부스팅 (HistGradientBoosting)**

- 기존 그래디언트 부스팅보다 더 빠르고 강력한 성능 제공
- 데이터를 256개 구간으로 나누어 연산 최적화
- LightGBM, XGBoost 등과 유사한 방식

```python
from sklearn.experimental import enable_hist_gradient_boosting
from sklearn.ensemble import HistGradientBoostingClassifier

# 히스토그램 기반 그래디언트 부스팅 모델 생성
hgb = HistGradientBoostingClassifier(random_state=42)

# 교차 검증 수행
scores = cross_validate(hgb, train_input, train_target, return_train_score=True, n_jobs=-1)

# 훈련 세트 & 테스트 세트 점수 출력
print(np.mean(scores['train_score']), np.mean(scores['test_score']))

# 결과 : 0.9321723946453317, 0.8801241948619236
```

→ 기존 그래디언트 부스팅보다 빠르고 높은 성능 달성

히스토그램 기반 그레이디언트 부스팅 모델을 훈련

```python
from sklearn. inspection import permutation_importance
hgb. fit(train_input, train_target)
result = permutation_importance(hgb, train_input, train_target,
n_repeats=10, random_state=42, n_jobs=-1)
print(result. importances_mean)

# 결과 : [0.08876275, 0.23438522, 0.08027708]
```

→ 평균을 출력해 보면 랜덤 포레스트와 비슷한 비율임을 알 수 있음

특성 중요도 계산

```python
result = permutation_ importancehgb, test_ input, test_target,
n_repeats=10, random_state=42, n_jobs=-1)
print(result. importances_mean)

# 결과 : [0.05969231, 0.20238462, 0.049]
```

→ 그래디언트 부스팅과 비슷하게 조금 더 당도에 집중하고 있다는 걸 알 수 있음

테스트세트 확인

```python
hgb.score(test_input, test_target)

# 결과 : 0.8723076923076923
```

→ 테스트 세트에서는 약 87% 정확도를 얻음

---

## **7. XGBoost & LightGBM**

XGBoost와 LightGBM은 **그래디언트 부스팅을 최적화한 라이브러리**

- **XGBoost**: `tree_method='hist'` 설정하여 히스토그램 방식 적용
- **LightGBM**: 빠른 속도와 높은 정확도로 인기

```python
from xgboost import XGBClassifier

xgb = XGBClassifier(tree_method='hist', random_state=42)
scores = cross_validate(xgb, train_input, train_target, return_train_score=True, n_jobs=-1)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))

# 결과 :  0.8827690284750664, 0.8708899089361072
```

```python
from lightgbm import LGBMClassifier

lgb = LGBMClassifier(random_state=42)
scores = cross_validate(lgb, train_input, train_target, return_train_score=True, n_jobs=-1)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))

# 결과 : 0.9338079582727165, 0.8789710890649293
```

- LightGBM이 XGBoost보다 빠른 속도를 제공