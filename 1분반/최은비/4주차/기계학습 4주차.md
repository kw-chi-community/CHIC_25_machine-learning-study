# 기계학습 4주차

# **04-1 로지스틱 회귀**

## **1. 로지스틱 회귀란?**

- 로지스틱 회귀(Logistic Regression)는 **분류(Classification) 모델**
- 선형 회귀와 비슷하지만, **출력이 연속적인 값이 아니라 확률(0~1 사이 값)을 반환**
- 특정 클래스로 분류될 확률을 예측하는 방식
- **시그모이드 함수(Sigmoid function)** 를 사용하여 출력값을 0과 1 사이의 값으로 변환

---

## **2. 확률과 분류 개념 이해**

### **(1) 럭키백의 확률**

- 홍콩에서 럭키백 이벤트를 진행한다고 가정
- 럭키백에 들어갈 확률이 **크기, 높이, 두께** 같은 **7개 특성**을 기반으로 결정됨
- 특정 생선이 럭키백에 들어갈 확률을 예측하는 문제
    
    **⇒ 분류 문제**로 해결 가능
    

```python
import pandas as pd

# 데이터 불러오기
fish = pd.read_csv('https://bit.ly/fish_csv_data')
fish.head()
```

```python
# 고유한 어종 출력
print(pd.unique(fish['Species']))
```

출력:

```python
['Bream' 'Roach' 'Whitefish' 'Parkki' 'Perch' 'Pike' 'Smelt']
```

- 7개의 생선 종류 있음
- **타깃 데이터는 어종(Species), 입력 데이터는 길이(Length), 무게(Weight), 높이(Height) 등 사용**

---

## **3. 데이터 전처리**

- `Species`(어종)를 **타깃 데이터**로 설정
- 나머지 5개 특성을 **입력 데이터**로 변환
- 넘파이 배열(`to_numpy()`)로 변환하여 모델 학습에 활용

```python
# 입력 데이터 선택 (Weight, Length, Diagonal, Height, Width)
fish_input = fish[['Weight', 'Length', 'Diagonal', 'Height', 'Width']].to_numpy()

# 타깃 데이터 선택 (어종)
fish_target = fish['Species'].to_numpy()
```

### **훈련 세트 & 테스트 세트 분리**

```python
from sklearn.model_selection import train_test_split

train_input, test_input, train_target, test_target = train_test_split(
    fish_input, fish_target, random_state=42
)
```

### **데이터 표준화 (StandardScaler) 적용**

```python
from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
ss.fit(train_input)
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)
```

**⇒ 특성 값들을 표준 정규분포(평균 0, 표준편차 1)로 변환**

⇒ 거리 기반 알고리즘(KNN)이나 회귀 모델에서 중요한 과정

---

## **4. K-최근접 이웃(KNN)으로 분류 수행**

```python
from sklearn.neighbors import KNeighborsClassifier

kn = KNeighborsClassifier(n_neighbors=3)
kn.fit(train_scaled, train_target)

print(kn.score(train_scaled, train_target))  # 훈련 세트 정확도
print(kn.score(test_scaled, test_target))    # 테스트 세트 정확도
```

출력:

```python
0.8907563025210085
0.85
```

- KNN 모델의 정확도가 꽤 높음
- `predict()`를 사용하여 테스트 세트 샘플 5개에 대한 예측 수행

```python
print(kn.predict(test_scaled[:5]))
```

출력:

```python
['Perch' 'Smelt' 'Pike' 'Perch' 'Perch']
```

***⇒ 어떤 어종인지 정확하게 예측***

---

## **5. KNN 분류 확률 예측**

- `predict_proba()`를 사용하면 **각 클래스(어종)에 대한 확률 값 반환**
- `np.round()`로 소수점 4자리까지 출력

```python
import numpy as np

proba = kn.predict_proba(test_scaled[:5])
print(np.round(proba, decimals=4))
```

출력:

```python
[[0.    0.    0.    0.    1.    0.    0.   ]
 [0.    0.    0.    0.    0.    1.    0.   ]
 [0.    0.    0.    0.    0.    0.3333 0.6667]
 [0.    0.    0.    0.    1.    0.    0.   ]
 [0.    0.    0.    0.    1.    0.    0.   ]]
```

- **각 샘플의 확률 분포를 출력**
- 예를 들어, **세 번째 샘플은 Pike(33.33%), Smelt(66.67%) 확률로 예측됨**

---

## **6. 로지스틱 회귀(Logistic Regression) 모델 적용**

- **시그모이드 함수(sigmoid function) 사용**
- **출력값을 0~1 사이 값(확률)로 변환하여 분류 수행**

### **로지스틱 회귀 함수**

ϕ(z)=11+e−z\phi(z) = \frac{1}{1 + e^{-z}}

ϕ(z)=1+e−z1​

➡ z가 크면 **1에 가까운 값**, z가 작으면 **0에 가까운 값** 반환

```python
import numpy as np
import matplotlib.pyplot as plt

z = np.arange(-5, 5, 0.1)
phi = 1 / (1 + np.exp(-z))

plt.plot(z, phi)
plt.xlabel('z')
plt.ylabel('phi')
plt.show()
```

➡ **시그모이드 함수 그래프 출력**

![스크린샷 2025-01-29 23.08.52.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%204%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%2018a77f5ae6fa80bcb7d8f4bb2d1c6bc1/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2025-01-29_23.08.52.png)

### **로지스틱 회귀로 이진 분류 수행하기**

로지스틱 회귀는 **두 개의 클래스(이진 분류)** 문제를 해결할 때 유용함.

예제 데이터에서 **도미(Bream)와 빙어(Smelt)** 두 개의 생선을 분류하는 모델을 학습

**- True, False 값 변환 예제**

```python
import numpy as np

char_arr = np.array(['A', 'B', 'C', 'D', 'E'])
print(char_arr[[True, False, True, False, False]])  
# ['A' 'C']
```

- **도미(Bream)와 빙어(Smelt)만 분류할 데이터 준비**

```python
bream_smelt_indexes = (train_target == 'Bream') | (train_target == 'Smelt')
train_bream_smelt = train_scaled[bream_smelt_indexes]
target_bream_smelt = train_target[bream_smelt_indexes]
```

- **로지스틱 회귀 모델 훈련 및 예측**

```python
from sklearn.linear_model import LogisticRegression

lr = LogisticRegression()
lr.fit(train_bream_smelt, target_bream_smelt)

# 샘플 5개에 대한 예측
print(lr.predict(train_bream_smelt[:5]))

# 결과 : ['Bream' 'Smelt' 'Bream' 'Bream' 'Bream']
```

- **각 클래스의 예측 확률 확인**

```python
print(lr.predict_proba(train_bream_smelt[:5]))

# 결과 : [[0.99759855 0.00240145]
#       [0.02735183 0.97264817]
#       [0.99486072 0.00513928]
#       [0.98584202 0.01415798]
#       [0.99767269 0.00232731]]
```

- **모델의 계수 확인**

```python
print(Lr.coef_, lr.intercept_
#  결과 : [[-0.4037798 -0.57620209 0.66280298 1.01290277 0.73168947]] [-2.16155132]
```

---

## 7. 로지스틱 회귀로 다중 분류 수행하기

```python
lr = LogisticRegression(C=20, max_iter=1000)
Lr. fit(train_scaled, train_target)
print(Ir.score(train_scaled, train_target))
print(lr.score (test_scaled, test_target))]
# 결과 : 0.9327731092436975
#       0.925

print(lr.predict(test_scaled[:5]))
# 결과 : ['Perch' 'Smelt' 'Pike' 'Roach' 'Perch']

proba = 1r. predict_proba(test_scaled[:5])
print(np.round (proba, decimals=3))
# 결과 : [[0. 0.014 0.841 0. 0.136 0.007 0.003]
#        [0. 0.003 0.044 0. 0.007 0.946 0. ]
#        [0. 0. 0.034 0.935 0.015 0.016 0. ]
#        [0.011 0.034 0.306 0.007 0.567 0. 0.076]
#        [0. 0. 0.904 0.002 0.089 0.002 0.001]]

print(lr.classes_)
# 결과 : ['Bream' 'Parkki' 'Perch' 'Pike' 'Roach' 'Smelt' 'Whitefish']

print(r.coef_.shape, r . intercept_.shape)
# 결과 : (7, 5 ) (7,)

decision = Ir.decision_function(test_scaled[:5])
print(np. round(decision, decimals=2))
# 결과 : [[-6.5 1.03 5.16 - 2 . 7 3 3.34 0.33 -0.63]
#        [-10.86 1.93 4.77 -2.4 2.98 7.84 -4.26]
#        [ -4.34 -6.23 3.17 6.49 2.36 2.42 3.87]
#        [ - 0 . 6 8 0.45 2.65 -1.19 3.26 5.75 1.26]
#        [ - 6 . 4 -1.99 5.82 -0.11 3.5 -0.11 -0.71]]
```

### **다중 클래스 분류 (Softmax 함수 사용)**

- **소프트맥스 함수(Softmax function)** 사용
- **7개 어종 각각의 확률값 계산**
- 확률이 가장 높은 클래스로 분류

```python
from scipy.special import softmax

decision = lr.decision_function(test_scaled[:5])
proba = softmax(decision, axis=1)
print(np.round(proba, decimals=4))
```

출력:

```python
[[0.   0.   0.   0.   1.   0.   0.  ]
 [0.   0.   0.   0.   0.   1.   0.  ]
 [0.   0.   0.   0.   0.   0.333 0.667]
 [0.   0.   0.   0.   1.   0.   0.  ]
 [0.   0.   0.   0.   1.   0.   0.  ]]

```

⇒ **소프트맥스를 이용한 확률 계산 완료**

```python
distances, indexes = kn.kneighbors(test_scaled[3:4])
print(train_target[indexes])

# 결과 : [['Roach' 'Perch' 'Perch']]
```

---

# **04-2 확률적 경사 하강법**

확률적 경사 하강법(SGD)은 머신러닝 모델을 훈련할 때 **경사 하강법(Gradient Descent)** 을 수행하는 방법 중 하나.

전체 데이터가 아니라 **확률적으로 선택한 일부 데이터를 이용하여 모델을 업데이트** 하기 때문에 빠르고 메모리 효율적.

## **1. 경사 하강법(Gradient Descent) 개념**

기본적인 경사 하강법은 **손실 함수(Loss Function)** 의 값을 최소화하는 방향으로 가중치를 조정하는 알고리즘이다.

이를 통해 모델의 예측 성능을 점진적으로 개선한다.

- **경사 하강법의 기본 원리**
1. 손실 함수(오차)가 가장 작은 지점을 찾는 것이 목표
2. 손실 함수의 기울기(미분 값)를 구해 방향을 결정
3. 일정한 학습률(learning rate)만큼 가중치를 업데이트
4. 위 과정을 반복하여 최적의 가중치(모델 학습 완료) 도출
- **경사 하강법의 문제점**
1. 데이터가 많을 경우 **연산량이 커지고 학습 속도가 느려질 수 있다**.
2. 지역 최적점(Local Minimum)에 빠질 가능성이 있다.
3. 미분 계산이 필요하므로 연속적인 손실 함수가 필요하다.

---

## **2. 확률적 경사 하강법(Stochastic Gradient Descent, SGD)**

확률적 경사 하강법은 전체 데이터를 사용하는 대신 **훈련 샘플을 무작위로 선택하여 가중치를 조정**하는 방법이다.

즉, 데이터 일부만 사용하므로 학습 속도가 빨라지고, 지역 최적점에서 벗어날 가능성이 높아진다.

- **SGD의 특징**
1. **빠르게 학습할 수 있다** (데이터 일부만 사용)
2. **계산량이 적어 메모리 사용이 적다**
3. **지역 최적점(Local Minimum)에서 탈출할 가능성이 높다**
4. 하지만 **수렴 속도가 불안정할 수 있다**
- **SGD의 과정**
    - 랜덤하게 샘플 하나 선택
    - 해당 샘플을 이용해 손실 함수의 기울기 계산
    - 가중치 업데이트
    - ㅣㄴ위 과정을 반복하여 최적의 가중치 찾기

---

## **3. 손실 함수(Loss Function) 개념**

손실 함수는 모델의 예측 값과 실제 값의 차이를 측정하는 함수이다.

경사 하강법에서는 이 손실 함수를 최소화하는 방향으로 학습이 진행된다.

- **손실 함수의 역할**
    - 예측 값이 실제 값과 얼마나 차이가 있는지 평가
    - 모델의 학습 방향 결정
    - 머신러닝에서 중요한 성능 지표
- **이진 분류(Binary Classification)에서의 손실 함수 - 로지스틱 손실 함수**

로지스틱 회귀에서는 **이진 분류 문제**를 해결하기 위해 **로그 손실 함수(Log Loss, Cross-Entropy Loss)** 를 사용한다.

- L=−(ylog(y^)+(1−y)log(1−y^))
    - y : 실제 레이블(0 또는 1)
    - y^ : 모델이 예측한 확률 값
- **로그 손실 함수 특징**
    - 예측이 정확할수록 손실 값이 작아짐
    - 예측이 틀릴수록 손실 값이 커짐
    - 경사 하강법을 사용할 때 미분 가능

---

## **4. 확률적 경사 하강법을 이용한 분류 모델 구현 (SGDClassifier 사용)**

사이킷런(sklearn)의 `SGDClassifier`를 사용하여 **SGD 기반의 분류 모델**을 학습시킨다.

여기서는 **물고기 데이터를 이용한 분류 모델을 학습**하는 과정을 구현한다.

- **데이터 불러오기 및 전처리**

```python
import pandas as pd

# 데이터 불러오기
fish = pd.read_csv('https://bit.ly/fish_csv_data')

# 입력 데이터와 타겟 데이터 분리
fish_input = fish[['Weight', 'Length', 'Diagonal', 'Height', 'Width']].to_numpy()
fish_target = fish['Species'].to_numpy()

# 훈련 세트와 테스트 세트로 나누기
from sklearn.model_selection import train_test_split

train_input, test_input, train_target, test_target = train_test_split(
    fish_input, fish_target, random_state=42
)

# 데이터 정규화 (표준화)
from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
ss.fit(train_input)
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)

```

- **SGDClassifier 모델 학습**

```python
from sklearn.linear_model import SGDClassifier

# SGD 기반의 분류 모델 생성
sc = SGDClassifier(loss='log', max_iter=10, random_state=42)
sc.fit(train_scaled, train_target)

# 모델 성능 평가
print(sc.score(train_scaled, train_target))  # 훈련 세트 점수
print(sc.score(test_scaled, test_target))  # 테스트 세트 점수

```

- **SGDClassifier 모델 반복 학습 (에포크 증가)**

```python
sc = SGDClassifier(loss='log', max_iter=100, tol=None, random_state=42)
sc.fit(train_scaled, train_target)

print(sc.score(train_scaled, train_target))  # 훈련 세트 점수
print(sc.score(test_scaled, test_target))  # 테스트 세트 점수

```

---

## **5. 배치 경사 하강법 vs 확률적 경사 하강법 vs 미니배치 경사 하강법**

| 방법 | 특징 | 장점 | 단점 |
| --- | --- | --- | --- |
| **배치 경사 하강법 (Batch Gradient Descent)** | 모든 데이터를 사용하여 기울기 계산 | 안정적으로 수렴 | 연산량이 많아 학습 속도가 느림 |
| **확률적 경사 하강법 (Stochastic Gradient Descent, SGD)** | 하나의 샘플을 랜덤하게 선택하여 기울기 계산 | 빠르게 학습, 지역 최적점 탈출 가능 | 수렴이 불안정 |
| **미니배치 경사 하강법 (Mini-batch Gradient Descent)** | 일부 샘플을 랜덤하게 선택하여 기울기 계산 | 속도와 안정성의 균형 | 배치 크기 설정이 중요 |

---

## **6. 에포크(Epoch)와 과적합/과소적합**

에포크(epoch)는 **전체 데이터를 한 번 학습하는 과정**을 의미한다.

에포크가 너무 작으면 **과소적합(underfitting)** 이 발생하고, 너무 크면 **과적합(overfitting)** 이 발생할 수 있다.

- **SGD에서 에포크 조정하기**

```python
import numpy as np
import matplotlib.pyplot as plt

sc = SGDClassifier(loss='log', random_state=42)
train_score = []
test_score = []
classes = np.unique(train_target)

# 300번 반복 학습
for _ in range(0, 300):
    sc.partial_fit(train_scaled, train_target, classes=classes)
    train_score.append(sc.score(train_scaled, train_target))
    test_score.append(sc.score(test_scaled, test_target))

# 정확도 그래프 출력
plt.plot(train_score)
plt.plot(test_score)
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.show()

```

- **최적의 에포크 설정 후 학습**

```python
sc = SGDClassifier(loss='log', max_iter=100, tol=None, random_state=42)
sc.fit(train_scaled, train_target)

print(sc.score(train_scaled, train_target))  # 훈련 세트 점수
print(sc.score(test_scaled, test_target))  # 테스트 세트 점수

```