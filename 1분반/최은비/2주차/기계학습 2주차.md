# 기계학습 2주차

# 02-2 데이터 전처리

### **데이터 전처리란?**

**기계 학습에서 데이터를 모델에 넣기 전에 데이터를 준비하는 필수 과정.(** 데이터를 **정리하고 변환**하는 과정.

만약 데이터 전처리를 하지 않으면

- **모델 성능 저하** : 데이터가 서로 다른 스케일(크기)일 경우, 특정 변수가 모델에 과도한 영향을 미칠 수 있음.
- **훈련 오류 발생** : 누락값, 중복값, 이상치 등이 있는 경우 모델이 제대로 학습하지 못할 수 있음.

- 정**규화(Normalization)**: 모든 데이터 값을 0과 1 사이로 변환.

**- 표준화(Standardization)**: 데이터의 평균을 0, 표준편차를 1로 맞춤.

**- 결측값 처리**: 누락된 값을 평균, 중앙값, 또는 다른 방식으로 대체.

**- 데이터 분리**: 학습용 데이터와 테스트 데이터를 나눠 모델 성능을 검증.

---

### **넘파이로 데이터 준비하기**

### **1) 물고기 길이와 무게 데이터**

- 물고기의 길이(fish_length)와 **무게(fish_weight)** 데이터를 리스트에 저장.

```python
fish_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0, 41.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 19.0, 21.0, 22.0, 23.0, 24.0]

fish_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 900.0, 955.0, 925.0, 975.0, 950.0, 950.0, 6.7, 7.5, 7.7, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 19.7, 19.9, 13.2, 14.1, 19.7, 19.9]
```

(기존에는 리스트를 순회하며 데이터를 구성했지만, 이제 넘파이를 사용해 더 간편하게 처리할 수 있음)

---

### **2) 데이터를 2차원 배열로 변환**

- 넘파이의 `column_stack()`으로 물고기 길이와 무게 데이터를 결합.

```python
import numpy as np
fish_data = np.column_stack((fish_length, fish_weight))

print(fish_datal:5])
```

- **결과 데이터 (`fish_data`)**
    
    ```python
    [[ 25.4, 242.0 ],
     [ 26.3, 290.0 ],
     [ 26.5, 340.0 ],
     [ 29.0, 363.0 ],
     [ 29.0, 430.0 ]]
    ```
    
    - 각 행은 `[길이, 무게]`로 나타남.

---

### **3) 타깃 데이터 생성**

- 물고기의 종류를 나타내는 타깃 데이터를 생성.
    - 빙어: `1`
    - 도미: `0`

```python
fish_target = np.concatenate((np.ones(35), np.zeros(14)))
print(fish_target)
```

- **결과 (`fish_target`)**
    
    ```python
    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
    ```
    

---

### **사이킷런으로 훈련 세트와 테스트 세트 나누기**

### **1) 데이터 분리**

- 훈련 세트와 테스트 세트로 나눔.
- `train_test_split()` 함수 사용.

```python
from sklearn.model_selection import train_test_split

train_input, test_input, train_target, test_target = train_test_split(
    fish_data, fish_target, random_state=42
)
```

### **2) 데이터 크기 확인**

- `train_input` (학습 데이터): `(36, 2)` (36개 샘플, 각 샘플 2개 특성)
- `test_input` (테스트 데이터): `(13, 2)`

```python
print(train_input.shape, test_input.shape)
# 결과 : (36, 2) (13, 2)

print(train_target.shape, test_target.shape)
# 결과 : (36,) (13,)

print(test_target)
# 결과 : [1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]
```

### **3) 문제점**

- 테스트 세트의 도미(1)와 빙어(0)의 비율이 **3.3:1**로, 원래 비율(2.5:1)과 달라짐.
- 이는 **무작위로 데이터를 나눌 때 발생하는 샘플링 편향** 때문.
- 샘플링 편향이 생기면 특정 클래스의 샘플이 부족해 학습이나 평가가 제대로 이루어지지 않을 수 있음.

### 4) 해결방안

- `train_test_split()` 함수에서 **`stratify` 매개변수**를 사용.
    - `stratify`에 **타깃 데이터(fish_target)**를 전달하면, **클래스 비율을 유지**하며 데이터를 나눌 수 있음.
- 특히, **데이터가 작거나 특정 클래스의 샘플이 적을 때 유용**.

```python
train_input, test_input, train_target, test_target = train_test_split(
	fish_data, fish_target, stratify=fish_target, random_state=42)
	
print(test_target)
```

**결과 데이터 (`fish_data`)**

```python
[0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1]
```

---

### **K-최근접 이웃(KNN) 모델 학습과 평가**

### **1) 모델 학습**

- K-최근접 이웃(KNN) 모델 생성 후 학습.

```python
from sklearn.neighbors import KNeighborsClassifier

kn = KNeighborsClassifier()
kn.fit(train_input, train_target)
```

### **2) 모델 성능 평가**

- `test_input`과 `test_target`으로 모델 평가.

```python
print(kn.score(test_input, test_target))
# 결과: 1.0
```

- **결과**: 테스트 데이터를 100% 정확도로 분류.

### **3) 새로운 데이터 예측**

- 길이 25cm, 무게 150g인 물고기의 종류 예측.

```python
print(kn.predict([[25, 150]]))
# 결과: [0] (도미)
```

---

### **데이터 시각화**

### 1) 새 데이터

```python
import matplotlib.pyplot as plt

plt.scatter(train_input[:, 0], train_input[:, 1], label='train data')
plt.scatter(25, 150, marker='^', label='new data')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```

![스크린샷 2025-01-15 13.50.06.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%202%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%2017c77f5ae6fa8005b7a0eab819ec4ce5/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2025-01-15_13.50.06.png)

- 새로운 샘플 marker 매개변수를 `'^'`로 지정하여 삼각형으로 나타냄.
- 오른쪽 위로 뻗어있는 다른 도미 데이터에 더 가까우나 왜 빙어 데이터에 더 가깝다고 판단하는 것일까?

### 2) 가까운 데이터

```python
plt.scatter(train_input[:,0], train_input[:,1])
plt. scatter(25, 150, marker='^')
plt. scatter(train_input[indexes,0], train_input[indexes,1], marker='D')
plt. xlabel('length')
olt.ylabel('weight')
plt. show()
```

![스크린샷 2025-01-15 13.56.37.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%202%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%2017c77f5ae6fa8005b7a0eab819ec4ce5/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2025-01-15_13.56.37.png)

- marker=D' 로 지정하면 산점도를 마름모로 그림( 삼각형 샘플에 가장 가까운 5 개의 샘플이 초록 다이아몬드로 표시)
- 예측 결과대로 가장 가까운 이웃에 도미가 하나밖에 포함되지 않았음

### 3) 직접 데이터 확인

```python
print(train_input[indexes])

# 결과 : [[[ 25.4, 242.0 ]
#         [ 15.0,  19.9 ]
#         [ 14.3,  19.7 ]
#         [ 13.0,  12.2 ]
#         [ 12.2,  12.2 ]]]

print(train_target[indexes])
# 결과 : [[1, 0, 0, 0, 0]]

print (distances)
# 결과 : [[ 92.00086956. 13048375378, 130.73859415, 138.32150953, 138.39320793]]
```

### 4) 기준 맞추기

![스크린샷 2025-01-15 14.08.36.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%202%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%2017c77f5ae6fa8005b7a0eab819ec4ce5/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2025-01-15_14.08.36.png)

- 거리가 92 와 130 이라고 했을 때 그래프에 나타난 거리 비율이 이상함.
- x축은 범위가 좁고 (10~40), y 축은 범위가 넓음 (0~1000).
- 이를 명확히 보기 위해  x축의 범위를 동일하게 0~1,000 으로 설정.

```python
plt.scatter(train_input[:,0], train_input[:,1])
olt. scatter (25, 150, marker=*^*)
plt.scatter(train_input[indexes,0], train_input[indexes,1], marker='D')
olt. xlim((0, 1000))
plt.xlabel('length')
plt.ylabel('weight')
plt.show)
```

![스크린샷 2025-01-15 14.13.49.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%202%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%2017c77f5ae6fa8005b7a0eab819ec4ce5/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2025-01-15_14.13.49.png)

---

### **스케일링(표준화)**

### **1) 표준화의 필요성**

- 길이(2541)와 무게(242975)의 값 차이가 크다.
- 스케일 차이는 거리 기반 모델(KNN)의 성능에 영향을 미침.
- 데이터의 평균과 표준편차를 기준으로 값을 변환.

### **2) 평균과 표준편차 계산**

```python
mean = np.mean(train_input, axis=0)
std = np.std(train_input, axis=0)

print(mean, std)
```

- **평균 (`mean`)**: `[27.29722222, 454.09722222]`
- **표준편차 (`std`)**: `[9.98244253, 323.29893931]`

### **3) 표준화 변환(브로드캐스팅)**

- 각 데이터에서 평균을 빼고 표준편차로 나눔.

```python
train_scaled = (train_input - mean) / std
```

### **4)전처리된 데이터 시각화**

- 데이터 표준화를 적용한 `train_scaled` 데이터와 새로운 샘플([25, 150])을 시각화.

```python
plt.scatter(train_scaled[:,0], train_scaled[:,1])
plt.scatter (25, 150, marker='^')
plt.xlabel('length' )
plt.ylabel( weight')
plt.show()
```

![스크린샷 2025-01-15 14.20.46.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%202%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%2017c77f5ae6fa8005b7a0eab819ec4ce5/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2025-01-15_14.20.46.png)

### **5) 새로운 데이터 표준화**

- 새로운 데이터([25, 150])를 학습 데이터의 평균과 표준편차로 표준화

```python
new = ([25, 150] - mean) / std
plt.scatter(train_scaled[:,0], train_scaled[:,1])
plt.scatter(new[0], new[1], marker='^')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```

![스크린샷 2025-01-15 14.22.08.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%202%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%2017c77f5ae6fa8005b7a0eab819ec4ce5/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2025-01-15_14.22.08.png)

### 6) 모델 학습과 테스트

- **훈련 데이터**와 **표준화된 데이터**를 이용해 모델 학습.
    
    ```python
    kn.fit(train_scaled, train_target)
    ```
    
- 테스트 데이터를 표준화하여 모델 평가.
    
    ```python
    test_scaled = (test_input - mean) / std
    print(kn.score(test_scaled, test_target))  
    # 정확도 1.0
    ```
    

새 데이터 예측

- 표준화된 새 데이터를 이용해 물고기의 종류를 예측.

```python
print(kn.predict([new]))  
# 결과 : [1] (도미)
```

### 7) 거리 기반 이웃 확인

- 새로운 샘플의 가장 가까운 이웃 시각화.

```python
distances, indexes = kn.kneighbors([new])
plt.scatter(train_scaled[:, 0], train_scaled[:, 1])
plt.scatter(new[0], new[1], marker='^')
plt.scatter(train_scaled[indexes, 0], train_scaled[indexes, 1], marker='D')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```

![스크린샷 2025-01-15 14.27.08.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%202%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%2017c77f5ae6fa8005b7a0eab819ec4ce5/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2025-01-15_14.27.08.png)

- 가장 가까운 샘플은 모두 도미 ⇒ 예측 성공