# 기계학습 6주차

# **06-1 군집 알고리즘**

- **흑백 과일 사진을 군집화하여 자동 분류하는 과정**을 다룬다.
- **비지도 학습(unsupervised learning)과 군집(Clustering) 알고리즘**을 이해한다.

---

## **1. 비지도 학습이란?**

- **타겟(정답) 데이터 없이** 패턴을 찾아 그룹을 형성하는 기법
- 대표적인 방법: **군집 알고리즘(Clustering Algorithm)**
- **K-평균(K-Means), 계층 군집(Hierarchical Clustering), DBSCAN** 등이 있음

**→ 군집 알고리즘을 활용하면 사람이 직접 분류하지 않아도 데이터의 패턴을 분석하여 자동으로 그룹을 만들 수 있음.**

---

## **2. 과일 사진 데이터 준비하기**

과일 사진 데이터셋에는 **사과, 바나나, 파인애플**이 포함되어 있음.

데이터는 **흑백 이미지(100 × 100 크기)로 저장된 `.npy` 파일**로 제공됨.

### **데이터 다운로드**

```python
!wget https://bit.ly/fruits_300_data -O fruits_300.npy
```

- `wget` 명령어를 사용하여 `.npy` 파일을 다운로드한다.

---

## **📌 3. 데이터 로드 및 확인하기**

### **numpy를 사용하여 데이터 불러오기**

```python
import numpy as np
import matplotlib.pyplot as plt

# 데이터 로드
fruits = np.load('fruits_300.npy')

# 데이터 크기 확인
print(fruits.shape)
```

**출력 예시**

```python
(300, 100, 100)
```

→ 300개의 샘플이 있고, 각 이미지의 크기는 **100 × 100 픽셀**

---

## **4. 첫 번째 이미지 출력하기**

```python
plt.imshow(fruits[0], cmap='gray')
plt.show()

```

출력 결과 : 첫 번째 이미지(**사과)**

![스크린샷 2025-02-08 오후 1.04.30.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%206%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%2019477f5ae6fa80deaa7af802ce48d67e/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2025-02-08_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_1.04.30.png)

**픽셀 값 확인**

```python
print(fruits[0, 0, :])

[ 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1
  2 2 2 2 2 2 1 1 1 1 1 1 1 1 2 3 2 1
  2 1 1 1 1 2 1 3 2 1 3 1 4 1 2 5 5 5
  19 148 192 117 28 1 1 2 1 4 1 1 3 1 1 1 1 1
  2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
  1 1 1 1 1 1 1 1 1 1 ]
```

→ 이미지의 첫 번째 행의 픽셀 값이 출력됨 (0~255 범위의 값)

---

## **5. 여러 이미지 비교하기**

```python
plt.imshow(fruits[0], cmap='gray_r')
plt.show
```

출력 결과 : 

![스크린샷 2025-02-08 오후 1.06.06.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%206%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%2019477f5ae6fa80deaa7af802ce48d67e/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2025-02-08_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_1.06.06.png)

```python
fig, axs = plt.subplots(1, 2)
axs[0].imshow(fruits[100], cmap='gray_r')  # 파인애플
axs[1].imshow(fruits[200], cmap='gray_r')  # 바나나
plt.show()
```

**출력 결과 :** 사과, 파인애플, 바나나 이미지가 출력됨.

![스크린샷 2025-02-08 오후 1.06.39.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%206%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%2019477f5ae6fa80deaa7af802ce48d67e/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2025-02-08_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_1.06.39.png)

---

## **6. 픽셀 값 분석하기**

픽셀 값의 평균을 계산하여 특징을 추출한다.

각 이미지를 1차원 벡터(10,000개 픽셀)로 변환한다.

### **이미지 데이터 변환**

```python
apple = fruits[0:100].reshape(-1, 100*100)  # 사과
pineapple = fruits[100:200].reshape(-1, 100*100)  # 파인애플
banana = fruits[200:300].reshape(-1, 100*100)  # 바나나

print(apple.shape)  # (100, 10000)
```

**출력 결과**

```python
(100, 10000)
```

→ 100개의 샘플이 각각 **10,000개 픽셀로 변환됨**

---

## **7. 평균 픽셀 값 계산**

```python
print(apple.mean(axis=1))
```

**출력 결과** (각 샘플의 평균값)

```python
[ 88.3346 97.9249 87.3709 98.3703 92.8705 82.6439 94.4244 95.5999
  90.681 81.6226 87.0578 95.0745 93.8416 87.017 97.507 87.2019
  88.9827 100.9158 92.7823 100.9184 104.9854 88.674 99.5643 97.2495
  94.1179 92.1935 95.1671 93.3322 102.8967 94.6695 90.5285 89.0744
  97.7641 97.2938 100.7564 90.5236 100.2542 85.8452 96.4615 97.1492
  90.711 102. 3193 87. 1629 89.8751 86.7327 86.3991 95.2865 89. 1709
  96.8163 91.6604 96. 1065 99.6829 94.9718 87.4812 89.2596 89.5268
  93.799 97.3983 87.151 97.825 103.22 94.4239 83.6657 83.5159
  102.8453 87.0379 91.2742 100.4848 93.8388 90.8568 97.4616 97.5022
  82.446 87.1789 96.9206 90.3135 90.565 97.6538 98.0919 93.6252
  87.3867 84.7073 89.1135 86.7646 88.7301 86.643 96.7323 97.2604
  81.9424 87.1687 97.2066 83.4712 95.9781 91.8096 98.4086 100.7823
  101.556 100.7027 91.6098 88.8976]
```

→ **사과 픽셀 값은 평균적으로 90~100 사이**

**히스토그램을 사용하여 분포 시각화**

```python
plt.hist(np.mean(apple, axis=1), alpha=0.8)
plt.hist(np.mean(pineapple, axis=1), alpha=0.8)
plt.hist(np.mean(banana, axis=1), alpha=0.8)
plt.legend(['apple', 'pineapple', 'banana'])
plt.show()
```

**출력 결과**

![스크린샷 2025-02-08 오후 1.31.25.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%206%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%2019477f5ae6fa80deaa7af802ce48d67e/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2025-02-08_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_1.31.25.png)

→ **사과는 90~100 바나나는 40 이하, 파인 애플은 90~100 사이**

→ 픽셀 값의 평균을 보면 **사과와 파인애플이 비슷하고 바나나는 확연히 다름**

```python
fig, axs = plt.subplots(1, 3, figsize=(20,5))
axs[0]. bar(range(10000), np.mean(apple, axis=0))
axs[1].bar (range(10000), np.mean(pineapple, axis=0))
axs [2].bar (range(10000), np.mean (banana, axis=0))
plt.show()
```

출력 결과

![스크린샷 2025-02-08 오후 1.36.02.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%206%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%2019477f5ae6fa80deaa7af802ce48d67e/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2025-02-08_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_1.36.02.png)

→ 순서대로 사과, 파인애플, 바나나 그래프임

→ 3개의 그래프를 보면 과일마다 값이 높은 구간이 다름

---

## **8. 평균값과 가까운 사진 찾기**

```python
apple_mean = np.mean(apple, axis=0).reshape(100, 100)
pineapple_mean = np.mean(pineapple, axis=0).reshape(100, 100)
banana_mean = np.mean(banana, axis=0).reshape(100, 100)

fig, axs = plt.subplots(1, 3, figsize=(20, 5))
axs[0].imshow(apple_mean, cmap='gray_r')
axs[1].imshow(pineapple_mean, cmap='gray_r')
axs[2].imshow(banana_mean, cmap='gray_r')
plt.show()
```

**출력 결과**

![스크린샷 2025-02-08 오후 1.37.56.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%206%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%2019477f5ae6fa80deaa7af802ce48d67e/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2025-02-08_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_1.37.56.png)

→ 각 과일의 평균 이미지를 확인 가능

---

## **9. 평균값과 가장 가까운 사진 찾기**

```python
abs_diff = np.abs(fruits - apple_mean)
abs_mean = np.mean(abs_diff, axis=(1, 2))

print(abs_mean.shape)  # (300,)
```

**출력 결과**

```python
(300,)
```

→ 각 샘플의 평균과 얼마나 차이가 나는지 계산

**가장 가까운 100개 샘플 찾기**

```python
apple_index = np.argsort(abs_mean)[:100]  # 가장 가까운 100개 샘플 선택

fig, axs = plt.subplots(10, 10, figsize=(10, 10))
for i in range(10):
    for j in range(10):
        axs[i, j].imshow(fruits[apple_index[i*10 + j]], cmap='gray_r')
        axs[i, j].axis('off')
plt.show()
```

**출력 결과**

![스크린샷 2025-02-08 오후 1.38.34.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%206%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%2019477f5ae6fa80deaa7af802ce48d67e/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2025-02-08_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_1.38.34.png)

→ **가장 가까운 100개의 사과 이미지를 자동으로 찾음**

---

# **06-2 k-평균(K-Means)**

- **비지도 학습을 이용해 과일 사진을 자동으로 분류하는 모델을 만든다.**
- **K-평균 군집화(K-Means Clustering) 알고리즘을 이해하고 구현한다.**

---

## **1. K-평균 알고리즘 소개**

- *K-평균(K-Means)**은 데이터를 **K개의 그룹(클러스터)**으로 나누는 군집 알고리즘이다.

이 과정에서 각 클러스터의 중심을 **클러스터 중심(centroid)**이라고 한다.

### **K-평균 알고리즘 동작 과정**

1. **무작위로 K개의 클러스터 중심을 설정**
2. 각 데이터 샘플을 **가장 가까운 클러스터 중심에 할당**
3. 클러스터 중심을 새롭게 업데이트
4. **클러스터 중심이 더 이상 변화하지 않을 때까지 2~3 과정을 반복**
- **반복(iteration)**을 통해 최적의 클러스터 중심을 찾는다.
- **K-평균은 반복할수록 클러스터 중심이 점점 더 정확해진다.**

---

## **2. 데이터 준비하기**

과일 사진 데이터는 **300개 흑백 이미지(100×100 크기)**로 제공됨.

우리는 **K-평균 알고리즘을 사용하여 사과, 바나나, 파인애플을 자동으로 분류**할 것이다.

**데이터 다운로드 및 로드**

```bash
!wget https://bit.ly/fruits_300_data -O fruits_300.npy
```

```python
import numpy as np

# npy 파일 로드
fruits = np.load('fruits_300.npy')

# 2차원 배열로 변환 (샘플 개수, 픽셀 수)
fruits_2d = fruits.reshape(-1, 100*100)
```

→ **각 이미지를 1차원 벡터(10,000픽셀)로 변환하여 K-평균 알고리즘에 적용할 준비 완료!**

```python
from sklear.cluster import Keans
‹m = KMeans(n_clusters=3, random_state=42)
km. fit(fruits_2d)
```

---

## **3. K-평균 클러스터링 적용**

```python
from sklearn.cluster import KMeans

# K-평균 모델 생성 (클러스터 개수: 3개)
km = KMeans(n_clusters=3, random_state=42)

# 데이터에 모델 적용 (훈련)
km.fit(fruits_2d)
```

- **K-평균 모델이 3개의 클러스터를 자동으로 찾음.**
- **각 샘플이 어느 클러스터에 속하는지 `km.labels_`에 저장됨.**

---

## **4. 클러스터링 결과 확인**

```python
print(km.labels_)  # 각 샘플의 클러스터 할당 결과 출력
print(np.unique(km.labels_, return_counts=True))  # 클러스터별 샘플 개수 확인
```

**출력 예시**

```python
[ 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 20 0 0 0 0 0 0 0 2 0 2 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 11 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ]
```

```python
(array([0, 1, 2]), array([ 91,  98, 111]))
```

→ **0번 클러스터: 91개, 1번 클러스터: 98개, 2번 클러스터: 111개 샘플이 속함**

---

## **5. 클러스터별 이미지 시각화**

```python
import matplotlib.pyplot as plt

def draw_fruits(arr, ratio=1):
    n = len(arr)  # 샘플 개수
    rows = int(np.ceil(n / 10))  # 10개씩 행을 나눔
    cols = min(10, n)  # 최대 10개 컬럼
    fig, axs = plt.subplots(rows, cols, figsize=(cols * ratio, rows * ratio))

    for i in range(rows):
        for j in range(cols):
            if i * 10 + j < n:
                axs[i, j].imshow(arr[i * 10 + j], cmap='gray_r')
                axs[i, j].axis('off')

    plt.show()
```

- **클러스터별 이미지를 그리는 함수 생성**
- 샘플 개수에 따라 **행/열을 자동으로 조절하여 시각화**

---

## **6. 클러스터별 이미지 출력**

```python
draw_fruits(fruits[km.labels_ == 0])  # 0번 클러스터 샘플 출력
draw_fruits(fruits[km.labels_ == 1])  # 1번 클러스터 샘플 출력
draw_fruits(fruits[km.labels_ == 2])  # 2번 클러스터 샘플 출력
```

**출력 결과**

- **0번 클러스터 → 사과**
    
    ![스크린샷 2025-02-08 오후 2.05.18.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%206%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%2019477f5ae6fa80deaa7af802ce48d67e/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2025-02-08_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_2.05.18.png)
    
- **1번 클러스터 → 바나나**
    
    ![스크린샷 2025-02-08 오후 2.06.01.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%206%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%2019477f5ae6fa80deaa7af802ce48d67e/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2025-02-08_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_2.06.01.png)
    
- **2번 클러스터 → 파인애플**
    
    ![스크린샷 2025-02-08 오후 2.06.16.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%206%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%2019477f5ae6fa80deaa7af802ce48d67e/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2025-02-08_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_2.06.16.png)
    

**K-평균 알고리즘이 자동으로 과일을 분류하는 것을 확인할 수 있음!**

---

## **7. 클러스터 중심 확인**

```python
draw_fruits(km.cluster_centers_.reshape(-1, 100, 100), ratio=3)
```

**출력 결과**

![스크린샷 2025-02-08 오후 2.06.41.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%206%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%2019477f5ae6fa80deaa7af802ce48d67e/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2025-02-08_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_2.06.41.png)

- K-평균이 최종적으로 찾은 **3개의 클러스터 중심(centroids) 출력**
- 각 클러스터 중심이 **사과, 바나나, 파인애플의 평균 이미지처럼 보임**

---

## **8. 새로운 이미지 예측하기**

```python
print(km. transform(fruits_2d[100:101]))
print(km.predict(fruits_2d[100:101]))  # 100번째 이미지의 클러스터 예측
```

**출력 예시**

```python
[[5267.70439881 8837.37750892 3393.8136117 ]]
[2]
```

→ **100번째 과일은 2번 클러스터(파인애플)로 분류됨!

샘플 확인**

```python
draw_fruits(fruits[100: 101])
```

출력 결과 : 

![스크린샷 2025-02-08 오후 2.08.39.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%206%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%2019477f5ae6fa80deaa7af802ce48d67e/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2025-02-08_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_2.08.39.png)

알고리즘의 반복 횟수(n_*iter* 속성에 저장)

```python
print(km.n_iter_)
```

출력 결과 : 

```python
3
```

---

## **9. 최적의 K 찾기 (엘보우 방법)**

K-평균 알고리즘을 사용할 때 **적절한 K 값(클러스터 개수)을 찾는 것이 중요**하다.

이를 위해 **엘보우 방법(Elbow Method)**을 사용한다.

**엘보우 방법 적용**

```python
inertia = []

for k in range(2, 7):  # K=2~6까지 반복
    km = KMeans(n_clusters=k, random_state=42)
    km.fit(fruits_2d)
    inertia.append(km.inertia_)  # 이너셔(inertia) 저장

# 결과 시각화
plt.plot(range(2, 7), inertia)
plt.xlabel('k')
plt.ylabel('inertia')
plt.show()
```

**출력 결과**

![스크린샷 2025-02-08 오후 2.12.41.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%206%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%2019477f5ae6fa80deaa7af802ce48d67e/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2025-02-08_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_2.12.41.png)

- **이너셔(Inertia)**: 클러스터 내 데이터들이 중심에서 얼마나 떨어져 있는지 나타내는 값
- 그래프에서 **이너셔가 급격히 감소하는 지점이 최적의 K 값**

**K=3에서 이너셔 감소 폭이 줄어드는 것을 확인 → K=3이 최적의 값으로 보임!**

---

# **06-3 주성분 분석 (PCA)**

- **차원 축소(Dimensionality Reduction)의 개념을 이해하고 PCA를 활용하여 데이터 차원을 줄이는 방법을 배운다.**
- **PCA를 활용해 데이터 저장 공간을 절약하고, 모델 학습 속도를 높이는 방법을 실습한다.**

---

## **1. 차원과 차원 축소 개념**

- **차원(Dimension)**: 데이터가 가진 특성(Feature)의 개수
- **차원 축소(Dimensionality Reduction)**:
    - 데이터의 차원을 줄여 저장 공간을 절약하고, 학습 속도를 향상시키는 방법
    - 중요한 정보는 유지하면서 불필요한 정보는 제거
- **예제: 과일 사진 데이터**
    - 원본 데이터: **10,000개 픽셀(100×100 크기의 이미지)**
    - 차원 축소 필요: **10,000개 차원을 유지하는 것은 비효율적**
    - *주성분 분석(PCA)**를 사용하여 차원을 줄이면서 정보 손실을 최소화

---

## **2. 주성분 분석(PCA) 개념**

- **PCA (Principal Component Analysis, 주성분 분석)**
    - *데이터의 분산이 가장 큰 방향(주성분)**을 찾아 데이터를 새로운 축으로 변환
    - 변환된 축을 기준으로 **가

장 중요한 특징을 유지하면서 차원을 줄임**

**PCA의 핵심 개념**

1. **데이터의 가장 큰 분산 방향(주성분)을 찾는다.**
2. **새로운 좌표계(주성분 축)로 데이터를 변환한다.**
3.  **가장 중요한 주성분만 남기고 나머지는 버려 차원을 축소한다.**
- **주성분 축은 데이터의 분포를 가장 잘 표현하는 방향으로 설정됨**
- **차원 축소 후에도 원본 데이터의 정보를 최대한 보존하도록 설계**

---

## **3. PCA를 활용한 데이터 차원 축소 실습**

### **1) 데이터 다운로드 및 로드**

```bash
!wget https://bit.ly/fruits_300_data -O fruits_300.npy
```

```python
import numpy as np
import matplotlib.pyplot as plt

# npy 파일 로드
fruits = np.load('fruits_300.npy')

# 2차원 배열로 변환 (샘플 개수, 픽셀 수)
fruits_2d = fruits.reshape(-1, 100*100)
```

→ **각 이미지를 10,000픽셀의 1차원 벡터로 변환하여 PCA에 적용할 준비 완료!**

---

### **2) PCA 모델 적용 (50개 주성분 선택)**

```python
from sklearn.decomposition import PCA

# PCA 적용 (주성분 개수: 50개)
pca = PCA(n_components=50)
pca.fit(fruits_2d)
```

**PCA 모델이 가장 중요한 50개의 주성분(Principal Components)을 학습**

```python
print(pca.components_.shape)  # (50, 10000)
```

**출력 결과**

```python
(50, 10000)
```

→ **10,000개의 픽셀을 50개의 주성분으로 압축 완료!**

**그림 그리기**

```python
draw_fruits(pca.components_.reshape(-1, 100, 100))
```

**출력 결과 :** 

![스크린샷 2025-02-08 오후 2.35.28.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%206%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%2019477f5ae6fa80deaa7af802ce48d67e/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2025-02-08_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_2.35.28.png)

---

### **4) 데이터 차원 축소**

```python
print(fruits_2d.shape)   # (300, 10000)
```

```python
# 원본 데이터를 PCA로 변환 (차원 축소)
fruits_pca = pca.transform(fruits_2d)

print(fruits_pca.shape)  # (300, 50)
```

**출력 결과**

```python
(300, 50)
```

→ **10,000차원의 데이터가 50차원으로 줄어듦!**

- **저장 공간 절약 & 모델 학습 속도 향상 효과**

---

### **5) 원본 데이터 복원 (재구성)**

```python
# PCA로 차원 축소된 데이터를 다시 원본 형태로 변환
fruits_inverse = pca.inverse_transform(fruits_pca)

print(fruits_inverse.shape)  # (300, 10000)
```

**출력 결과**

```python
(300, 10000)
```

→ **50개의 주성분만 사용했지만, 원본 데이터와 유사한 형태로 복원 가능!**

- 하지만 일부 정보 손실이 발생할 수 있음
- 차원 축소 후에도 데이터를 최대한 유지하는 것이 PCA의 핵심 목표

---

### **6) 원본 vs 재구성 데이터 비교**

```python
# 복원된 데이터 시각화
fruits_reconstruct = fruits_inverse.reshape(-1, 100, 100)

for start in [0, 100, 200]:  # 3가지 샘플 비교
    draw_fruits(fruits_reconstruct[start:start+100])
    print("\n")
```

**출력 결과**

![스크린샷 2025-02-08 오후 2.41.57.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%206%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%2019477f5ae6fa80deaa7af802ce48d67e/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2025-02-08_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_2.41.57.png)

- **원본 데이터와 유사한 과일 이미지들이 재구성됨**
- 일부 블러(Blur) 현상이 발생하지만, 데이터의 핵심 정보는 유지됨

---

## **4. PCA의 설명된 분산 (Explained Variance)**

```python
print(np.sum(pca.explained_variance_ratio_))  # 설명된 분산 비율 출력
```

**출력 결과**

```python
0.9215298262612741
```

- **92%의 데이터 정보를 유지한 상태에서 차원 축소 성공**

### **설명된 분산 그래프 시각화**

```python
plt.plot(pca.explained_variance_ratio_)
plt.show()
```

**출력 결과**

![스크린샷 2025-02-08 오후 2.43.20.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%206%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%2019477f5ae6fa80deaa7af802ce48d67e/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2025-02-08_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_2.43.20.png)

- **첫 10개 주성분이 대부분의 분산을 설명하고 있음**
- 불필요한 특성을 제거하면서도 **데이터 정보 손실을 최소화**

---

## **5. PCA + 분류 모델 적용 (Logistic Regression)**

PCA를 활용해 차원을 줄인 데이터로 **로지스틱 회귀(Logistic Regression) 모델을 학습**

### **1) 타겟 데이터 생성**

```python
from sklearn.linear_model import LogisticRegression

# 사과=0, 파인애플=1, 바나나=2 라벨링
target = np.array([0]*100 + [1]*100 + [2]*100)

# 로지스틱 회귀 모델 생성
lr = LogisticRegression()
```

---

### **2) 원본 데이터 학습 (교차 검증)**

```python
from sklearn.model_selection import cross_validate

# 원본 데이터(10,000차원)로 교차 검증 수행
scores = cross_validate(lr, fruits_2d, target)

# 정확도 출력
print(np.mean(scores['test_score']))
print(np-mean(scores['fit_time']))
```

**출력 결과**

```python
0.9966666666666667
0.9422160625457764
```

→ **고차원 데이터(10,000차원)로 학습하면 매우 높은 성능이 나옴!**

---

### **3) PCA 데이터 학습 (교차 검증)**

```python
# PCA 변환된 데이터(50차원)로 학습
scores = cross_validate(lr, fruits_pca, target)

# 정확도 출력
print(np.mean(scores['test_score']))
print(np.mean(scores[' fit_time']))
```

**출력 결과**

```python
1.0
0.03256878852844238
```

→ **50개 주성분만 사용해도 100% 정확도 달성!**

---

→ **차원 축소를 통해 데이터 효율성을 극대화할 수 있음

주성분을 찾도록 하는 PCA 모델**

```python
pca = PCA(n_components=0.5) # 설명된 분산(variance)의 50%를 유지하는 주성분 개수 자동 선택
pca. fit(fruits_2d) #  PCA 모델을 데이터에 맞춰 학습

print(pca.n_components_) # **최적의 주성분 개수를 자동으로 찾아 출력**
```

**출력 결과** 

```python
2
```

- **50%의 분산을 유지하기 위해 주성분 개수 2개를 선택함**
- **즉, 데이터의 차원을 2개로 축소하여 원본 정보를 최대한 유지하도록 변환됨**

---

**원본 데이터를 주성분 공간으로 변환**

```python
fruits_pca = pca. transform(fruits_2d) # **원본 데이터를 PCA 변환하여 차원을 축소**
print(fruits_pca.shape) # **변환된 데이터의 크기 출력**
```

출력 결과

```python
(300, 2)
```

- **300개의 샘플이 이제 2개의 차원으로 표현됨**
- **원래 (300, 10000)의 데이터가 (300, 2)로 줄어들어 연산량이 크게 감소**

---

**교차 검증을 통한 모델 성능 평가**

```python
scores = cross_validate(lr, fruits_pca, target) # 로지스틱 회귀(Logistic Regression) 모델을 사용하여 교차 검증 수행
print(np.mean(scores['test_score'])) # 교차 검증에서 모델의 평균 정확도 출력
print(np.mean(scores['fit_time'])) # 모델 학습에 걸린 평균 시간 출력
```

**출력 결과**

```python
0.9933333333333334
0.04122166633605957
```

- **PCA를 사용하여 차원을 줄였음에도 모델 정확도가 99.3%로 매우 높음**
- **모델 학습 속도(`fit_time`)도 0.04초로 매우 빨라짐 → 원본 데이터보다 빠르게 학습 가능**

---

## **6. PCA + K-Means 군집화 적용**

```python
from sklearn.cluster import KMeans

# K-평균 클러스터링 적용 (차원 축소된 데이터 사용)
km = KMeans(n_clusters=3, random_state=42)
km.fit(fruits_pca)

print(np.unique(km.labels_, return_counts=True))  # 클러스터별 샘플 개수 확인
```

**출력 결과**

```python
(array([0, 1, 2]), array([91, 99, 110]))
```

→ **PCA를 적용한 데이터에서도 K-Means가 정확하게 과일을 군집화함!**

---

**군집화된 이미지 출력**

```python
for label in range(0, 3):
draw_fruits(fruits[km. labels_ == label])
# draw_fruits() 함수를 사용해 **각 클러스터별 이미지를 출력
#** km.labels_ == label을 사용하여 **각 군집에 속하는 과일 이미지를 필터링**

print("\n")
```

출력 결과

![스크린샷 2025-02-09 오전 11.31.31.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%206%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%2019477f5ae6fa80deaa7af802ce48d67e/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2025-02-09_%E1%84%8B%E1%85%A9%E1%84%8C%E1%85%A5%E1%86%AB_11.31.31.png)

- **왼쪽**: 동그란 형태 → 사과(apple)
- **가운데**: 길쭉한 모양 → 바나나(banana)
- **오른쪽**: 둥글지만 거친 표면 → 파인애플(pineapple)

---

**군집화된 데이터 산점도 출력**

```python
# K-Means 결과를 시각적으로 확인하기 위해 각 군집을 색상별로 구분
for Label in range(@, 3):
	data = fruits_pcalkm.labels_== LabeL]
	plt.scatter(data[:,0], data[:,1]) # PCA로 변환된 2차원 데이터를 사용하여 scatter()로 산점도 플롯

plt. legend(['apple', 'banana', 'pineapple']) # legend()를 사용해 군집별 라벨(사과, 바나나, 파인애플) 추가
plt. show()
```

**출력 결과**

![스크린샷 2025-02-09 오전 11.32.09.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%206%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%2019477f5ae6fa80deaa7af802ce48d67e/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2025-02-09_%E1%84%8B%E1%85%A9%E1%84%8C%E1%85%A5%E1%86%AB_11.32.09.png)

```python
scores = cross_validate(Ir, fruits_pca, target) # 
print(np.mean(scores[ test_score ] ) )
print(np.mean(scores['fit_time']))
```

- **차원 축소 후에도 K-Means가 과일을 정확히 분류했음을 시각적으로 확인 가능**
- **PCA를 사용하면 데이터가 2차원 공간에 잘 정리됨을 볼 수 있음**
    - **파란색 점 (apple)** → **왼쪽 하단에 모여 있음**
    - **주황색 점 (banana)** → **오른쪽에 넓게 분포**
    - **초록색 점 (pineapple)** → **왼쪽 상단에 위치**