# 기계학습 3주차

# **03-1 K-최근접 이웃 회귀**

## **1. 회귀 문제란?**

- `회귀(Regression)`는 주어진 입력 데이터(특징)를 기반으로 `연속적인 값(타깃)`을 예측하는 문제를 뜻함.
- `분류(Classification)`와 달리, 결과값이 `숫자(수치)`로 나타나는 것이 특징.

### **예시**

- 물고기의 길이를 이용해 **무게**를 예측.
- 집의 크기와 위치를 이용해 **집값**을 예측.

---

## **2. K-최근접 이웃 회귀란?**

- 새로운 데이터를 예측할 때 **가장 가까운 k개의 데이터를 선택**하고, 그들의 **평균값**을 예측값으로 반환하는 방식.

### **특징**

1. **거리 기반 알고리즘**:
    - 입력 데이터의 스케일(값의 크기)이 클수록 거리에 영향을 미침.
    - 따라서, `스케일 조정(정규화 또는 표준화)`이 매우 중요.
2. **이웃의 수(k) 설정 중요성**:
    - **k가 작을수록**:
        - 모델이 **개별 데이터에 민감** → 과적합(Overfitting) 위험 증가.
    - **k가 클수록**:
        - **전체 데이터를 반영** → 과소적합(Underfitting) 위험 증가.
3. **회귀 방식**:
    - 가장 가까운 **k개의 이웃의 평균값**을 예측값으로 사용.

---

## **3. 결정계수 R^2**

- R^2는 회귀 모델의 성능을 평가하는 지표.
- 값의 범위:
    - R^2 = 1: 완벽한 예측.
    - R^2 = 0: 예측이 데이터의 평균과 동일
    - R^2 < 0: 모델 성능이 평균보다 나쁨.

### **수식**

![스크린샷 2025-01-22 15.23.58.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%203%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%2018377f5ae6fa80c48046d5cde1bf9724/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2025-01-22_15.23.58.png)

- **SSR (오차제곱합)**: 예측값과 실제값 간의 차이의 제곱합.
- **TSS (총변동)**: 전체 데이터의 분산(평균과 실제값 차이의 제곱합).

---

## **4. 데이터 준비**

### **1) 물고기 데이터 정의**

- 물고기의 `길이(특징)`와 **`무게(타깃)`** 데이터를 배열로 저장.

```python
import numpy as np

# 물고기의 길이 (특징 데이터)
perch_length = np.array(
	[8.4, 13.7, 15.0, 16.2, 18.0, 19.6, 20.0, 21.0, 22.0, 22.5, 23.0, 24.0, 24.5, 25.0, 25.6, 26.5, 27.0, 27.6, 28.0, 28.7,  30.0, 31.0, 31.5, 32.0, 32.8, 33.5, 34.0, 34.5, 35.0, 36.5, 36.6, 37.0, 37.5, 38.0, 39.0, 39.5, 40.0, 40.5, 41.0, 41.5, 42.0, 42.5, 43.0, 43.5, 44.0]
)

# 물고기의 무게 (타깃 데이터)
perch_weight = np.array(
	[5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 85.0, 85.0, 110.0, 115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 130.0, 150.0, 160.0, 169.0, 200.0, 210.0, 215.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1000.0]
)
```

---

### **2) 데이터 시각화**

- 물고기의 길이와 무게 데이터를 산점도로 시각화하여 관계를 확인.

```python
import matplotlib.pyplot as plt

plt.scatter(perch_length, perch_weight)
plt.xlabel('length')  # x축: 물고기 길이
plt.ylabel('weight')  # y축: 물고기 무게
plt.show()
```

![스크린샷 2025-01-22 15.28.16.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%203%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%2018377f5ae6fa80c48046d5cde1bf9724/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2025-01-22_15.28.16.png)

---

## **5. 데이터 분리 및 전처리**

### **1) 훈련 데이터와 테스트 데이터 분리**

- 데이터를 **훈련 데이터**와 **테스트 데이터**로 분리.
- `train_test_split` 함수 사용.

```python
from sklearn.model_selection import train_test_split

train_input, test_input, train_target, test_target = train_test_split(
    perch_length, perch_weight, random_state=42
)
```

### **2) 데이터 형태 변환 (1차원 → 2차원)**

- 사이킷런은 **2차원 데이터**를 입력으로 요구하므로, 데이터를 변환.

```python
train_input = train_input.reshape(-1, 1)
test_input = test_input.reshape(-1, 1)
print(train_input.shape, test_input.shape)

# 결과 : (42, 1) (14, 1)
```

---

## **6. 모델 학습**

### **1) K-최근접 이웃 회귀 모델 생성**

- 사이킷런의 `KNeighborsRegressor` 클래스를 사용해 모델을 생성하고 학습.

```python
from sklearn.neighbors import KNeighborsRegressor

# 모델 생성
knr = KNeighborsRegressor()

# 모델 학습
knr.fit(train_input, train_target)
```

---

## **7. 모델 평가**

### **1) 결정계수 R^2 계산**

- 테스트 데이터를 사용해 R2 점수 계산.

```python
print(knr.score(test_input, test_target))

# 결과 : 0.9928094061010639
```

### **2) 평균 절대 오차 (MAE) 계산**

- 테스트 데이터의 예측값과 실제값 간의 평균 절대 오차를 계산.

```python
from sklearn.metrics import mean_absolute_error

# 테스트 데이터 예측값
test_prediction = knr.predict(test_input)

# 평균 절대 오차 계산
mae = mean_absolute_error(test_target, test_prediction)
print(mae)

# 결과 : 19.157142857142862

print(knr.score(train_input, train_target))
# 결과 : 0.9698823289099255

# 훈련 세트에서 점수가 굉장히 좋았는데 테스트 세트에서는 점수가 굉장히 나쁘다면 모델이 훈련 세트에 과대적합 weriting 되었다고 말함.즉 훈련 세트에만 잘 맞는 모델이라 테스트 세트와 나중에 실전에 투입하여 새로운 샘플에 대한 예측을 만들 때 잘 동작하지 않음.
# 훈련 세트보다 테스트 세트의 점수가 높거나 두 점수가 모두 너무 낮은 경우모델이 훈련 세트에 과소적합 되었다고 말함. 즉 모델이 너무 단순하여 훈련 세트에 적절히 훈련되지 않은 경우.
```

- 훈련 세트보다 테스트 세트의 점수가 높으니 과소적합임.
- 이러한 문제를 어떻게 해결할 수 있을까?

### **3) 이웃 수(k) 변경 (해결방안)**

- `n_neighbors` 값을 변경하여 모델 성능을 조정.
- 아래는 `k=3`으로 설정한 예제.

```python
# 이웃의 수를 3으로 설정
knr.n_neighbors = 3

# 모델 재학습
knr.fit(train_input, train_target)

# 성능 확인
print(knr.score(test_input, test_target))

# 결과 : 0.974645996398761
```

---

# **03-2 선형 회귀**

## **1. 선형 회귀란?**

- `선형 회귀(Linear Regression)`는 데이터를 가장 잘 설명하는 **직선 방정식**을 찾아 예측값을 반환하는 회귀 기법.
- 직선 방정식의 일반적인 형태:
    
                                           **y=ax+b**
    
    - y: 예측값(타깃)
    - x: 입력값(특징)
    - a: 기울기(coefficient)
    - b: 절편(intercept

---

## 2. 데이터 준비

### 1) 데이터 정의

- 물고기의 `길이(특징)`와 **`무게(타깃)`** 데이터를 배열로 저장.

```python
import numpy as np

# 물고기의 길이 (특징 데이터)
perch_length = np.array(
	[8.4, 13.7, 15.0, 16.2, 18.0, 19.6, 20.0, 21.0, 22.0, 22.5, 23.0, 24.0, 24.5, 25.0, 25.6, 26.5, 27.0, 27.6, 28.0, 28.7,  30.0, 31.0, 31.5, 32.0, 32.8, 33.5, 34.0, 34.5, 35.0, 36.5, 36.6, 37.0, 37.5, 38.0, 39.0, 39.5, 40.0, 40.5, 41.0, 41.5, 42.0, 42.5, 43.0, 43.5, 44.0]
)

# 물고기의 무게 (타깃 데이터)
perch_weight = np.array(
	[5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 85.0, 85.0, 110.0, 115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 130.0, 150.0, 160.0, 169.0, 200.0, 210.0, 215.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1000.0]
)
```

### 2) 2차원 배열로 변환

```python
from sklearn.model_selection import train_test_split

# 훈련 세트와 테스트 세트로 나눕니다
train_input, test_input, train_target, test_target = train_test_split(
perch_length, perch_weight, random_state=42
)

# 훈련 세트와 테스트 세트를 2 차원 배열로 바꿉니다
train_input = train_input. reshape(-1, 1)
test_input = test_input.reshape(-1, 1)
```

### 3) 모델 훈련

```python
from sklearn.neighbors import KNeighborsRegressor
knr = KNeighborsRegressor(n_neighbors=3)

# k- 최근접 이웃 회귀 모델을 훈련합니다
knr. fit(train_input, train_target)
```

---

## **3. K-최근접 이웃 회귀에서의 한계**

### **1) 문제 상황**

- 농어의 **길이**를 통해 **무게**를 예측.
- k-최근접 이웃 회귀를 사용했을 때 **길이 50cm 농어**의 무게를 예측하면 다음과 같음:

```python
print(knr.predict([[50]]))
# 결과: [1033.33333333]
```

- 실제로는 50cm 농어의 무게가 **1,033g보다 훨씬 높다**는 것을 관찰.
- 이는 k-최근접 이웃 회귀 모델이 새로운 데이터에 대한 일반화 능력이 부족하기 때문.

---

### **2) 한계 원인**

- k-최근접 이웃 회귀는 **주변 데이터의 평균**을 사용하여 예측.
- 훈련 데이터 범위 바깥의 값을 예측할 때 적합하지 않음(훈련 데이터에 의존적).

---

### 3-1) 산점도 표시

```python
import matplotlib.pyplot as plt

#50cm 농어의 이웃을 구합니다
distances, indexes = knr.kneighbors([[50]])

# 훈련 세트의 산점도를 그립니다
plt.scatter(train_input, train_target)

# 훈련 세트 중에서 이웃 샘플만 다시 그립니다
plt.scatter(train_input[indexes], train_target[indexes], marker='D')

# 50cm 농어 데이터
plt.scatter(50, 1033, marker='^')
plt. xlabel(' length')
plt. ylabel('weight')
plt. show)
```

![스크린샷 2025-01-22 16.01.12.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%203%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%2018377f5ae6fa80c48046d5cde1bf9724/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2025-01-22_16.01.12.png)

- 길이가 50cm 이고 무게가 1,033g인 농어는 (marker='^') 으로 표시되고 그 주변의 샘플은 (marker=’D') 으로 표시 됨.
- 산점도를 보면 길이가 커질수록 농어의 무게가 증가하는 경향이 있음.
- 하지만 50cm 농어에서 가장 가까운 것은 45cm 근방임

---

### 3-2) 이웃샘플의 타깃의 평균

```python
print(np.mean (train_ target[indexes]))

# 결과: [1033.33333333]
```

- 한 번 더 그래프를 그려 확인을 해보면

```python
# 100cm 농어의 이웃을 구합니다
distances, indexes = knr.kneighbors([[100]])

# 훈련 세트의 산점도를 그립니다
plt.scatter(train_input, train_target)

# 훈련 세트 중에서 이웃 샘플만 다시 그립니다
plt.scatter(train_input[indexes], train_target[indexes], marker='D')

# 100cm 농어 데이터
plt.scatter (100, 1033, marker='^')
plt. xlabel('length')
plt. ylabel('weight')
olt. show(
```

![스크린샷 2025-01-22 16.06.57.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%203%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%2018377f5ae6fa80c48046d5cde1bf9724/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2025-01-22_16.06.57.png)

---

## **4. 선형 회귀로 문제 해결**

### **1) 선형 회귀 모델 학습**

- 선형 회귀 모델을 사용해 데이터를 학습.

```python
from sklearn.linear_model import LinearRegression

# 선형 회귀 모델 생성 및 학습
lr = LinearRegression()
lr.fit(train_input, train_target)

# 50cm 농어의 무게 예측
print(lr.predict([[50]]))

# 결과: [1241.83886233]
```

- 선형 회귀 모델은 길이 50cm 농어의 무게를 **1,241g**으로 예측, 실제 값에 더 가까움.

---

### **2) 직선 방정식 확인**

- 선형 회귀 모델은 다음 방정식을 학습:
                                               **y=ax+b**
    - a: 기울기 (lr.coef_)
    - b: 절편 (lr.intercept_)

```python
print(lr.coef_, lr.intercept_)
# 결과: [39.01714496] -709.0186449535477
```

---

### **3) 데이터 시각화**

**훈련 데이터와 직선 방정식 그래프**

```python
# 훈련 세트의 산점도를 그립니다
plt.scatter(train_input, train_target)

# 15에서 50까지 1차 방정식 그래프를 그립니다
plt.plot([15, 50], [15*lr.coef_ + lr.intercept_, 50*lr.coef_ + lr.intercept_])

# 50cm 농어 데이터
plt.scatter(50, 1241.8, marker='^')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```

![스크린샷 2025-01-22 16.12.53.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%203%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%2018377f5ae6fa80c48046d5cde1bf9724/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2025-01-22_16.12.53.png)

- 그래프에서 직선 방정식이 데이터를 잘 설명하며, 새로운 데이터(50cm 농어)에 대한 예측값도 시각적으로 확인 가능.

---

### 4) R^2 점수 확인

```python
print(Lr.score(train_input, train_target)) # 훈련 세트
print(lr.score(test_input, test_target)) # 테스트 세트

# 결과 : 0.9398463339976039
#       0.8247503123313558
```

---

## **5. 선형 회귀의 문제점**

- 선형 회귀는 **모든 데이터를 직선으로 표현**하려고 함.
- 하지만, 농어의 길이와 무게는 선형적이지 않고, **비선형적 관계**가 존재.

### **문제 상황 예시**

- 길이가 100cm인 농어의 무게를 예측하면:

```python
print(lr.predict([[100]]))
# 결과: [3292.37233185]
```

- 예측값이 **실제 무게보다 훨씬 과대 추정**됨.

---

## **6. 다항 회귀 (Polynomial Regression)**

### **1) 다항 회귀란?**

- 데이터가 **비선형 관계**일 경우, 직선 방정식이 아닌 **다항식**을 사용하여 더 복잡한 관계를 설명.
- 다항식 방정식의 일반적인 형태:
                                     **y=ax^2 + bx + c**

---

### **2) 데이터 변환 (특징 추가)**

- 기존 데이터를 **다항식 특징**으로 변환.
- x → [x,x^2]와 같이 변환하여 모델에 추가 정보를 제공.

```python
# 다항식 특징 추가
train_poly = np.column_stack((train_input**2, train_input))
test_poly = np.column_stack((test_input**2, test_input))

print(train_poly.shape, test_poly.shape)
# 결과: (42, 2) (14, 2)
```

---

### **3) 다항 회귀 모델 학습**

- 선형 회귀 모델을 그대로 사용하되, 변환된 데이터를 학습.

```python
lr = LinearRegression()
lr.fit(train_poly, train_target)

# 50cm 농어의 무게 예측
print(lr.predict([[50**2, 50]]))

# 결과: [1573.98423528]
```

- 다항 회귀 모델은 50cm 농어의 무게를 **1,573g**으로 예측, 더 정확한 결과를 제공.

훈련한 계수와 절편 출력

```python
print(lr.coef_, lr.intercept_)
# 결과 : [1.01433211, 21.55792498] 116.0502107827827
```

---

### **4) 그래프 시각화**

**2차 방정식 그래프 그리기**

```python
# 구간별 직선을 그리기 위해 15에서 49까지의 정수 배열을 만듭니다
point = np.arange(15, 50)

# 훈련 세트의 산점도를 그립니다
plt.scatter(train_input, train_target)  

# 15에서 49까지 2차 방정식 그래프를 그립니다
plt.plot(point, 1.01*point**2 - 21.6*point + 116.05)  

# 50cm 농어 데이터
plt.scatter(50, 1574, marker='^')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```

![스크린샷 2025-01-22 16.21.27.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%203%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%2018377f5ae6fa80c48046d5cde1bf9724/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2025-01-22_16.21.27.png)

- 그래프에서 2차 방정식이 데이터를 잘 설명하며, 새로운 데이터에 대해 더 신뢰할 수 있는 예측값을 반환.

---

## **7. 모델 평가**

### **1) 훈련 세트와 테스트 세트 점수 비교**

```python
print(lr.score(train_poly, train_target))  # 훈련 세트
print(lr.score(test_poly, test_target))  # 테스트 세트

# 결과: 0.9706807451768623 (훈련 세트)
#      0.9775935108325122 (테스트 세트)

```

- 다항 회귀 모델은 **훈련 세트와 테스트 세트 모두에서 높은 성능**을 보임.
- 선형 회귀 모델에 비해 **일반화 성능**이 더 우수.

---

## **8. K-최근접 이웃 vs 선형 회귀 vs 다항 회귀**

| **모델** | **장점** | **단점** |
| --- | --- | --- |
| **K-최근접 이웃** | 단순하고 이해하기 쉬움 | 훈련 데이터 범위 밖에서는 성능 저하 |
| **선형 회귀** | 빠르고 직선 관계 데이터를 잘 설명 | 비선형 데이터에서는 성능 저하 |
| **다항 회귀** | 비선형 데이터도 잘 설명 가능 | 모델 복잡도가 높아질 수 있음 |

---

# **03-3 특성 공학과 규제**

## **1. 특성 공학 (Feature Engineering)**

- **특성 공학**은 데이터에 존재하지 않는 새로운 특성을 추가하여 모델 성능을 개선하는 작업.
- 기존의 데이터를 변환하거나 조합하여 새로운 정보를 만들어냄.

### **예시**

- 농어의 **길이**, **두께**, **높이** 데이터를 조합하여 새로운 특성을 생성:
    - 예: 부피=길이×두께×높이.

---

## **2. 다중 회귀 (Multiple Regression)**

- 하나의 특성만 사용하는 **단순 회귀**와 달리, **여러 특성**을 사용하는 회귀 방식.
- 다중 회귀는 **선형 모델** 로 표현 가능:

                                y = a1x1 + a2x2 + ⋯ + b

- x1,x2 : 여러 개의 특성.
- a1,a2 : 각 특성의 계수(coefficient).
- b : 절편(intercept).

---

## **3. 데이터 준비**

### **1) CSV 파일 읽기**

- **pandas**를 사용해 데이터를 읽고 넘파이 배열로 변환.

```python
import pandas as pd # pd는 관례적으로 사용하는 판다스의 별칭
df = pd.read_csv('https://bit.ly/perch_csv_data')  # CSV 파일 읽기
perch_full = df.to_numpy()  # 넘파이 배열로 변환
print(perch_full)
```

- 출력된 데이터는 각 열이 농어의 **길이**, **높이**, **두께**를 포함.

---

### **2) 타깃 데이터 준비**

- 농어의 **무게** 데이터를 타깃으로 사용.

```python
import numpy as np
perch_weight = np.array(
	[5.9, 32.0, 40.0, 51.5, 70.0, 78.0, 85.0, 85.0, 110.0, 115.0, 125.0, 130.0, 130.0, 135.0, 130.0, 150.0, 160.0, 169.0, 200.0, 210.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1000.0]
	)
```

---

### **3) 훈련 세트와 테스트 세트 분리**

```python
from sklearn.model_selection import train_test_split
train_input, test_input, train_target, test_target = train_test_split(
    perch_full, perch_weight, random_state=42
)

```

---

## **4. 사이킷런 변환기 : PolynomialFeatures**

### **1) PolynomialFeatures 클래스**

- 다항식 특성을 자동으로 생성.
- 기존 특성 (x1​,x2​,...)에서 조합된 새로운 특성 (x12​,x1​x2​,...)을 추가.

```python
from sklearn.preprocessing import PolynomialFeatures

poly = PolynomialFeatures
poly. fit([[2, 31])
print(poly. transform([[2, 3]]))
# 결과 : [[1, 2, 3, 4, 6, 9]]

poly = PolynomialFeatures(include_bias=False)
oly. fit([[2, 3]])
print(poly. transform([[2, 3]]))
# 결과 : [[2, 3, 4, 6, 9]]

poly = PolynomialFeatures(include_bias=False)  # 편향항(b) 제외
poly.fit(train_input)
train_poly = poly.transform(train_input)

print(train_poly.shape)  # (42, 9)

```

---

### **2) 변환된 특성 확인**

```python
poly.get_feature_names()
# 출력: ['x0', 'x1', 'x2', 'x0^2', 'x0 x1', 'x0 x2', 'x1^2', 'x1 x2', 'x2^2']

#테스트 세트 변환
test_poly = poly. transform(test_ input)
```

- x0, x1, x2: 원본 특성.
- x0^2, x0x1, x1^2 : 새로운 다항식 특성.

### 3) 다중 회귀 모델 훈련

```python
from sklearn. linear_model import LinearRegression
lr = LinearRegression()
lr. fit(train_poly, train_target)

print(lr.score(train_poly, train_target))
# 결과 : 0. 9903183436982124

print(lr. score(test_poly, test_target)) # 테스트 세트 점수
# 결과 : 0.9714559911594134

#5제곱까지 특성을 만들어 출력
poly = PolynomialFeatures(degree=5, include_bias=False)
poly. fit(train_input)
train_poly = poly. transform(train_input)
test_poly = poly. transform(test_input)
print (train_poly.shape)
# 결과 : (42, 55)

# 선형회귀 모델을 다시 훈련

lr. fit(train_poly, train_target)
print(lr.score(train_poly, train_target))
# 결과 : 0.9999999999991898
print(lr. score(test_poly, test_target))
# 결과 : -144. 40579242684848
```

---

## **5. 규제 (Regularization)**

- **규제**는 과적합을 방지하기 위해 **모델의 복잡도를 줄이는 기법**.
- 회귀 모델에서 `계수(coefficient)`가 너무 크면 과적합이 발생하므로 이를 제어.

```python
from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
ss.fit(train_poly)
train_scaled = ss. transform(train_poly)
test_scaled = ss. transform(test_poly)
```

### **1) 릿지 회귀 (Ridge Regression)**

- **릿지 회귀**는 계수의 크기를 제곱하여 규제하는 방식.
- 릿지 회귀 방정식:
    
    ![스크린샷 2025-01-22 16.53.49.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%203%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%2018377f5ae6fa80c48046d5cde1bf9724/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2025-01-22_16.53.49.png)
    
    - RSS : 잔차 제곱합.
    - α : 규제 강도(값이 클수록 규제 증가).
    - wi : 각 계수.

### 2) 릿지 회귀 훈련

```python
from sklearn.linear_model import Ridge

ridge = Ridge
ridge.fit(train_poly, train_target)
print(ridge.score(train_poly, train_target))  # 훈련 세트 점수
# 결과 : 0.9896101671037343
print(ridge.score(test_poly, test_target))  # 테스트 세트 점수
# 결과 : 0.9790693977615398

#적절한 alpha 값을 찾는 한 가지 방법 : pha 값에 대한 R2 값의 그래프를 그려 보는 것

#플롯립을 임포트하고 alpha 값을 바꿀 때마다 score () 메서드의 결과를 저장할 리스트
Import matplotlib.pyplot as plt
train_score = []
test_score = []

#alpha 값을 0.001 에서 100 까지 10 배씩 늘려가며 릿지 회귀 모델을 훈련한 다음 훈련 세트와 테스트 세트의 점수를 파이썬 리스트에 저장
alpha_list = [0.001, 0.01, 0.1, 1, 10, 100]
for alpha in alpha_list:
	#릿지 모델을 만듭니다
	ridge = Ridge(alpha=alpha)
	# 릿지 모델을 훈련합니다
	ridge. fit(train_scaled, train_target)
	# 훈련 점수와 테스트 점수를 저장합니다
	train_score.append(ridge.score(train_scaled, train_target))
	test_score.append (ridge.score(test_scaled, test_target))

#그래프 그리기
plt.plot(np. 1og10(alpha_list), train_score)
plt.plot(np.log10(alpha_list), test_score)
plt.xlabel('alpha')
plt.ylabel('R^2')
plt.show()
```

![스크린샷 2025-01-22 17.02.18.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%203%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%2018377f5ae6fa80c48046d5cde1bf9724/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2025-01-22_17.02.18.png)

```python
ridge = Ridge(alpha=0.1)
ridge.fit(train_scaled, train_target)
print(ridge.score(train_scaled, train_target))
print(ridge.score(test_scaled, test_target))

# 결과 : 0. 9903815817570366
#       0.9827976465386926
```

---

### **2) 라쏘 회귀 (Lasso Regression)**

- **라쏘 회귀**는 계수의 크기 합을 규제.
- 라쏘 회귀 방정식:
    
    ![스크린샷 2025-01-22 16.55.11.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%203%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%2018377f5ae6fa80c48046d5cde1bf9724/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2025-01-22_16.55.11.png)
    

### 4) 라쏘 회귀 훈련

```python
from sklearn. linear_model import Lasso
lasso = Lasso)
lasso. fit(train_scaled, train_target)
print(lasso.score(train_scaled, train_target))
print(lasso.score(test_scaled, test_target))
# 결과 : 0. 9897898972080961
#       0.9800593698421883

# 점수 계산 #
in_score = []
test_score = []
alpha_list = [0.001, 0.01, 0.1, 1, 10, 100]
for alpha in alpha_list:
	# 라쏘 모델을 만듭니다
	lasso = Lasso(alpha=alpha, max_iter=10000)
	# 라쏘 모델을 훈련합니다
	lasso. fit(train_scaled, train_target)
	# 훈련 점수와 테스트 점수를 저장합니다
	train_score.append(lasso.score(train_scaled, train_target))
	test_score.append(lasso.score(test_scaled, test_target))
	

# train score 와 test_score 리스트를 사용해 그래프 그리기 #
plt.plot(np.log10(alpha_list), train_score)
plt.plot(np.log10(alpha_list), test_score)
plt. xlabel('alpha')
plt. ylabel('R^2')
plt.show()
```

![스크린샷 2025-01-22 17.06.13.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%203%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%2018377f5ae6fa80c48046d5cde1bf9724/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2025-01-22_17.06.13.png)

```python
lasso = Lasso(alpha=10)
lasso. fit(train_scaled, train_target)
print (lasso.score(train_scaled, train_target))
grint(lasso.score(test_scaled, test_target))

# 결과 : 0.9888067471131867
#       0. 9824470598706695
```

---

## **7. 한 눈 정리**

| **기법** | **특징** |
| --- | --- |
| **릿지 회귀** | 계수 제곱합을 규제, 모든 특성 사용 |
| **라쏘 회귀** | 계수 합을 규제, 일부 계수를 0으로 만듦 |
- **적절한 규제 강도(α\alphaα)를 설정**하면, 과적합을 방지하고 모델 성능을 최적화할 수 있음.