# **5-1. 결정트리 모델 & 지니 불순도**

## **지니 불순도에서 음성 클래스와 양성 클래스 비율에 제곱을 하는 이유**

- 지니 불순도(Gini Impurity) 공식:
  \[
  G = 1 - (p_1^2 + p_2^2)
  \]
  - \( p_1 \) = 음성 클래스(0) 비율
  - \( p_2 \) = 양성 클래스(1) 비율
- 제곱을 하는 이유:
  1. **미분 가능하게 만들기 위함**
     - 머신러닝 모델 학습 시, 미분이 가능한 형태가 필요함.
     - 제곱을 사용하면 **부드러운 곡선 형태를 가지므로 최적화에 유리**함.
  2. **순수한(한쪽 클래스가 지배적인) 노드에 대해 0에 가까운 값을 가지도록 하기 위함**
     - \( p_1 = 1 \), \( p_2 = 0 \)인 경우 → \( G = 1 - (1^2 + 0^2) = 0 \)
     - 즉, 노드가 순수해질수록 불순도 값이 작아짐.

---

## **결정트리 모델 - 부모 노드와 자식 노드의 불순도 차이(정보 이득)**

- **정보 이득(Information Gain)** 공식:
  \[
  \text{Information Gain} = G*{\text{parent}} - \sum (\frac{|N*{\text{child}}|}{|N*{\text{parent}}|} \times G*{\text{child}})
  \]
  - 부모 노드와 자식 노드의 **불순도 차이**가 커야 정보 이득이 큼.
  - 즉, **노드를 분할할 때 정보 이득이 최대화되도록 해야 함**.

---

## **Depth 값을 최대한 늘리는 것이 좋은가?**

- **Depth를 늘리면?**

  - 결정 트리는 깊이가 깊을수록 데이터에 **완벽하게 적합(Overfitting)**할 가능성이 커짐.
  - 훈련 데이터에서는 성능이 좋지만, 테스트 데이터에서는 성능이 나쁠 수 있음.

- **Depth와 정보 이득의 최적값을 찾아야 하는가?**
  - 맞음. 적절한 `max_depth`를 찾아야 **일반화 성능을 높일 수 있음**.
  - 보통 **하이퍼파라미터 튜닝**을 통해 최적값을 찾음 (`GridSearchCV`, `RandomizedSearchCV` 등 활용).

---

## **특성 중요도 (Feature Importance)**

- **특성 중요도(Feature Importance)란?**

  - 결정 트리가 **특정 특성이 불순도를 줄이는 데 얼마나 기여했는지**를 나타내는 값.
  - 중요도가 높은 특성일수록 **더 상위 노드에서 분할 기준으로 사용됨**.

- **특성 중요도를 판단하는 기준**
  1. **불순도 감소량**:
     - 해당 특성을 기준으로 데이터를 분할했을 때 **불순도가 얼마나 감소했는지** 측정.
  2. **정보 이득**:
     - 특정 특성을 이용해 분할했을 때 **정보 이득이 얼마나 증가하는지** 확인.
  3. **결정 경로의 기여도**:
     - 결정 트리에서 **특성이 전체 트리에서 얼마나 자주 사용되었는지**를 분석하여 중요도를 평가.

---

# **5-2. 데이터 분할 & 교차 검증 & 하이퍼파라미터 탐색**

## **훈련 데이터 수와 테스트/검증 세트의 비율**

- 보통 **20~30%를 테스트/검증 세트**로 사용한다고 하지만, **훈련 데이터가 많으면 더 적은 비율을 사용해도 문제 없음**.
- 즉, **훈련 데이터가 많아질수록 테스트/검증 데이터의 비율은 반비례할 수 있음**.
  - 예제:
    - 데이터셋 크기: 1,000 → 테스트/검증 데이터 20~30% 적절.
    - 데이터셋 크기: 1,000,000 → 테스트/검증 데이터 5~10%도 충분.

---

## **교차 검증을 하기 위해 각 세트별 비율을 맞춰야 하는가?**

- 일반적인 데이터 분할에서 **20~30%를 테스트 세트로 지정**하지만, 교차 검증(K-Fold)을 수행하면 매번 훈련 세트와 검증 세트가 변경됨.
- **해결책:** `StratifiedKFold`를 사용하면 **클래스 비율을 유지하면서 교차 검증 가능**.

```python
from sklearn.model_selection import StratifiedKFold

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
```

# **5-3. 트리의 앙상블**

## **1. 앙상블 학습이란?**

- 여러 개의 모델을 결합하여 성능을 향상시키는 기법.
- 개별 모델보다 강력하고 안정적인 예측이 가능함.
- 대표적인 방법: **랜덤 포레스트, 엑스트라 트리, 그래디언트 부스팅, 히스토그램 기반 그래디언트 부스팅**.

---

## **2. 주요 앙상블 학습 알고리즘**

### **2.1 랜덤 포레스트 (Random Forest)**

- 가장 대표적인 앙상블 학습 방법.
- 개별 결정 트리를 랜덤하게 학습하여 예측 성능을 향상.
- **특징**: 성능이 뛰어나고 안정적이며, 과적합 위험이 적음.
- **단점**: 학습 속도가 다소 느릴 수 있음.

### **2.2 엑스트라 트리 (Extra Trees)**

- 랜덤 포레스트와 유사하지만, 부트스트랩 샘플을 사용하지 않고 랜덤하게 노드를 분할.
- **특징**: 랜덤 포레스트보다 빠르지만, 트리가 많아야 효과적.

### **2.3 그래디언트 부스팅 (Gradient Boosting)**

- 기존 모델의 오차를 보완하는 방식으로 트리를 연속적으로 추가하여 학습.
- **특징**: 성능이 뛰어나지만, 병렬 처리에 적합하지 않아 학습 속도가 느릴 수 있음.
- **장점**: 학습률(learning rate)을 조정하여 모델의 복잡도를 제어 가능.

### **2.4 히스토그램 기반 그래디언트 부스팅**

- 데이터 분포를 히스토그램 형태로 변환하여 학습 속도를 극대화한 그래디언트 부스팅 방식.
- **특징**: 학습 속도가 매우 빠르며 대용량 데이터에 적합.
- **라이브러리**: XGBoost, LightGBM 등에서 구현되어 있음.

---

## **3. 앙상블 학습과 하이퍼파라미터 튜닝**

- 앙상블 학습 모델에서 **그리드 서치(Grid Search)** 및 **랜덤 서치(Random Search)**를 활용하여 최적의 성능을 찾을 수 있음.
- 이를 통해 머신러닝 모델의 성능을 극대화 가능.

---

## **4. 지도 학습 (Supervised Learning)**

- 입력과 타겟이 주어진 상태에서 문제를 해결하는 머신러닝 기법.
- 앙상블 학습도 지도 학습의 한 종류.

---

## **5. 결론**

- 앙상블 학습은 여러 개의 결정 트리를 조합하여 성능을 향상하는 기법.
- 다양한 방법이 존재하며, 데이터의 특성에 따라 적절한 모델을 선택해야 함.
- 하이퍼파라미터 튜닝을 활용하면 최적의 성능을 얻을 수 있음.
