# **5-1. 결정트리 모델 & 지니 불순도**

## **지니 불순도에서 음성 클래스와 양성 클래스 비율에 제곱을 하는 이유**

- 지니 불순도(Gini Impurity) 공식:
  \[
  G = 1 - (p_1^2 + p_2^2)
  \]
  - \( p_1 \) = 음성 클래스(0) 비율
  - \( p_2 \) = 양성 클래스(1) 비율
- 제곱을 하는 이유:
  1. **미분 가능하게 만들기 위함**
     - 머신러닝 모델 학습 시, 미분이 가능한 형태가 필요함.
     - 제곱을 사용하면 **부드러운 곡선 형태를 가지므로 최적화에 유리**함.
  2. **순수한(한쪽 클래스가 지배적인) 노드에 대해 0에 가까운 값을 가지도록 하기 위함**
     - \( p_1 = 1 \), \( p_2 = 0 \)인 경우 → \( G = 1 - (1^2 + 0^2) = 0 \)
     - 즉, 노드가 순수해질수록 불순도 값이 작아짐.

---

## **결정트리 모델 - 부모 노드와 자식 노드의 불순도 차이(정보 이득)**

- **정보 이득(Information Gain)** 공식:
  \[
  \text{Information Gain} = G*{\text{parent}} - \sum (\frac{|N*{\text{child}}|}{|N*{\text{parent}}|} \times G*{\text{child}})
  \]
  - 부모 노드와 자식 노드의 **불순도 차이**가 커야 정보 이득이 큼.
  - 즉, **노드를 분할할 때 정보 이득이 최대화되도록 해야 함**.

---

## **Depth 값을 최대한 늘리는 것이 좋은가?**

- **Depth를 늘리면?**

  - 결정 트리는 깊이가 깊을수록 데이터에 **완벽하게 적합(Overfitting)**할 가능성이 커짐.
  - 훈련 데이터에서는 성능이 좋지만, 테스트 데이터에서는 성능이 나쁠 수 있음.

- **Depth와 정보 이득의 최적값을 찾아야 하는가?**
  - 맞음. 적절한 `max_depth`를 찾아야 **일반화 성능을 높일 수 있음**.
  - 보통 **하이퍼파라미터 튜닝**을 통해 최적값을 찾음 (`GridSearchCV`, `RandomizedSearchCV` 등 활용).

---

## **특성 중요도 (Feature Importance)**

- **특성 중요도(Feature Importance)란?**

  - 결정 트리가 **특정 특성이 불순도를 줄이는 데 얼마나 기여했는지**를 나타내는 값.
  - 중요도가 높은 특성일수록 **더 상위 노드에서 분할 기준으로 사용됨**.

- **특성 중요도를 판단하는 기준**
  1. **불순도 감소량**:
     - 해당 특성을 기준으로 데이터를 분할했을 때 **불순도가 얼마나 감소했는지** 측정.
  2. **정보 이득**:
     - 특정 특성을 이용해 분할했을 때 **정보 이득이 얼마나 증가하는지** 확인.
  3. **결정 경로의 기여도**:
     - 결정 트리에서 **특성이 전체 트리에서 얼마나 자주 사용되었는지**를 분석하여 중요도를 평가.

---

# **5-2. 데이터 분할 & 교차 검증 & 하이퍼파라미터 탐색**

## **훈련 데이터 수와 테스트/검증 세트의 비율**

- 보통 **20~30%를 테스트/검증 세트**로 사용한다고 하지만, **훈련 데이터가 많으면 더 적은 비율을 사용해도 문제 없음**.
- 즉, **훈련 데이터가 많아질수록 테스트/검증 데이터의 비율은 반비례할 수 있음**.
  - 예제:
    - 데이터셋 크기: 1,000 → 테스트/검증 데이터 20~30% 적절.
    - 데이터셋 크기: 1,000,000 → 테스트/검증 데이터 5~10%도 충분.

---

## **교차 검증을 하기 위해 각 세트별 비율을 맞춰야 하는가?**

- 일반적인 데이터 분할에서 **20~30%를 테스트 세트로 지정**하지만, 교차 검증(K-Fold)을 수행하면 매번 훈련 세트와 검증 세트가 변경됨.
- **해결책:** `StratifiedKFold`를 사용하면 **클래스 비율을 유지하면서 교차 검증 가능**.

```python
from sklearn.model_selection import StratifiedKFold

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
```

# **5-3. 트리의 앙상블**
