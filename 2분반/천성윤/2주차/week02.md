# 데이터 전처리와 머신러닝

## **1. 데이터 전처리**

머신러닝 모델에 훈련 데이터를 주입하기 전에 데이터를 가공하는 단계.

- 모델에 주입되는 데이터 품질을 높이고 성능을 개선하기 위한 작업.
- 이상치(outlier) 제거, 결측치 처리, 스케일 조정 같은 작업들이 포함됨.
- 데이터를 그냥 넣으면 모델 성능이 나쁠 가능성이 크기 때문에 반드시 필요한 과정 !!

---

## **2. 표준점수 (Standard Score)**

- **정의**: 표준점수는 데이터의 스케일을 바꾸는 대표적인 방법.
- **계산 방법**:
  - 각 특성의 평균(mean)을 빼고 표준편차(std)로 나눠야 함.
  - 공식:
    \[
    z = \frac{x - \mu}{\sigma}
    \]
  - \(x\): 원본 데이터, \(\mu\): 평균, \(\sigma\): 표준편차
- **주의점**:
  - 반드시 **훈련 세트의 평균(mean)**과 **표준편차(std)**를 이용해서 **테스트 세트를 변환**해야 함.
  - 샘플 하나만으로는 평균과 표준편차를 구할 수 없음.

---

## **3. 브로드캐스팅 (Broadcasting)**

- **정의**: 크기가 다른 넘파이 배열에서 자동으로 사칙 연산을 확장하여 수행하는 기능.
- 작은 배열의 크기를 큰 배열의 크기에 맞춰 연산을 처리함.
- 추가 메모리 없이 계산 효율을 높여줌.
- 데이터를 특정 축 기준으로 연산해야 할 때 자주 사용됨.
- 데이터 전처리 과정에서 크기 조정 없이 여러 데이터를 연산할 때 유용함.
- **예시**:

  ```python
  import numpy as np

  # 1D 배열
  a = np.array([1, 2, 3])
  # 스칼라 값
  b = 2
  # 브로드캐스팅으로 모든 요소에 2를 곱함
  print(a * b)  # 출력: [2, 4, 6]
  ```

## 넘파이 주요 함수

### 4.1 np.column_stack

전달받은 리스트를 일렬로 세운 후, 차례대로 간단하게 연결.

```python
import numpy as np

a = [1, 2, 3]
b = [4, 5, 6]
c = np.column_stack((a, b))
print(c)

# 출력:
# [[1 4]
#  [2 5]
#  [3 6]]
```

### 4.2 np.concatenate

배열을 지정한 축을 따라 연결.

```python
a = np.array([[1, 2], [3, 4]])
b = np.array([[5, 6]])
c = np.concatenate((a, b), axis=0)
print(c)
# 출력:
# [[1 2]
#  [3 4]
#  [5 6]]
```

etc.
np.mean: 배열의 평균값 계산
np.std:배열의 표준편차 계산

### 5. 훈련 데이터 스케일 조정

    - 훈련 데이터를 평균으로 빼고 표준편차로 나누어 변환하면 값의 범위가 변경됨.

- 데이터 간 스케일 차이가 큰 경우 모델이 특정 특성에 과도하게 의존할 가능성을 줄여줌.
- **중요**: 반드시 훈련 세트에서 계산된 평균(mean)과 표준편차(std)를 사용해 테스트 세트를 변환해야 함.
- 동일한 기준으로 변환하지 않으면 모델이 데이터를 잘못 학습하거나 예측 오류가 커질 수 있음.

## **6. scikit-learn 주요 메서드 개념**

### **6.1 `kn.fit`**

- K-최근접 이웃(KNN) 모델을 훈련하는 메서드 (1주차 내용).
- 훈련 데이터를 입력으로 받아 모델을 학습시킴.
- 훈련 데이터의 패턴을 저장하고, 이후 예측에서 활용됨.

### **6.2 `kneighbors`**

- 특정 데이터 포인트의 최근접 이웃을 찾는 메서드.
- 지정된 포인트에 가장 가까운 이웃들의 거리와 인덱스를 반환함.
- 데이터의 유사성 또는 근접성을 확인하는 데 유용함 !!

---

## **7. 데이터 변환과 산점도**

- 데이터를 변환한 후, 동일한 기준으로 모든 데이터를 변환해야 정확한 분석이 가능함.
- 잘못된 기준으로 변환하면 산점도가 왜곡되고, 데이터 분포를 오해할 수 있음.
- 모든 데이터는 같은 기준으로 변환하고, 변환된 데이터를 기반으로 산점도를 그려야 함.

---

## **8. 핵심 패키지**

- **scikit-learn**: 머신러닝 모델 구현과 데이터 전처리에 필수적인 Python 라이브러리.
- 다양한 전처리, 분류, 회귀, 군집화 알고리즘을 지원하며 데이터 분석의 표준 도구로 사용됨.

---

## Quiz

- 표준점수 (Standard Score): 스케일 조정 방식 중 하나로 특성값을 0에서 표준편차의 몇배수만큼 떨어져있는지 변환한 값
- 테스트 스케일을 조정하려고 할 때, 어떤 데이터 통계 값을 사용해야 하나

1. 훈련세트 (정답)
2. 테스트세트
3. 전체 데이터

### 이유

1. 훈련 데이터의 기준 유지:
   • 머신러닝 모델은 훈련 데이터를 기반으로 학습합니다.
   • 따라서 테스트 데이터도 동일한 기준(훈련 세트의 평균과 표준편차)을 사용하여 변환해야 일관성이 유지됩니다.
2. 정보 누출 방지:
   • 테스트 세트를 기준으로 통계 값을 계산하면 정보 누출(Information Leakage) 문제가 발생합니다.
   • 테스트 데이터는 모델 평가를 위한 데이터이므로, 훈련 과정에서 사용해서는 안 됩니다.
3. 실제 데이터 환경 반영:
   • 전체 데이터를 기준으로 통계 값을 계산하면, 훈련 데이터에 없는 정보를 포함할 수 있어 실제 환경에서 동작하지 않을 수 있습니다.

# Question
